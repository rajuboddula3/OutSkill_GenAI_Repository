{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d7e2558",
   "metadata": {},
   "source": [
    "# Advanced Pandas: Complete Best Practices Guide\n",
    "\n",
    "## Comprehensive Coverage of Advanced Pandas Techniques\n",
    "\n",
    "This notebook demonstrates:\n",
    "- **Task 1**: Advanced Data Transformation and Reshaping\n",
    "- **Task 2**: Advanced Merging and Joining\n",
    "- **Task 3**: Performance Optimization & Memory Management\n",
    "- **Task 4**: Custom Extensions & Advanced Functionality\n",
    "- **Task 5**: Real-world Challenge - Retail Analytics\n",
    "\n",
    "### Key Features:\n",
    "✓ Clean code architecture with classes\n",
    "✓ Comprehensive docstrings and comments\n",
    "✓ Best practices for memory and performance\n",
    "✓ Production-ready error handling\n",
    "✓ Real-world data patterns and edge cases\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72847ad6",
   "metadata": {},
   "source": [
    "## 0. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c80bde3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas Version: 2.2.3\n",
      "NumPy Version: 2.2.5\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import uuid\n",
    "import time\n",
    "import warnings\n",
    "import string\n",
    "import random\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Advanced imports\n",
    "from functools import wraps\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', 100)      # Show max 100 rows\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.2f}')  # Format floats to 2 decimals\n",
    "\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a60b66",
   "metadata": {},
   "source": [
    "## 1. Data Generation Module\n",
    "\n",
    "### Overview\n",
    "Generate synthetic datasets for demonstration. We use a custom generator without external dependencies for maximum compatibility.\n",
    "\n",
    "### Key Points:\n",
    "- **Reproducibility**: Set random seeds for consistent results\n",
    "- **Realistic data**: Include missing values, duplicates, outliers\n",
    "- **Multiple datasets**: Transactions, products, customers, status updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a6553048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA GENERATOR CLASS - Lightweight alternative to Faker\n",
    "# ============================================================================\n",
    "\n",
    "class SimpleDataGenerator:\n",
    "    \"\"\"\n",
    "    Generate realistic fake data without external dependencies.\n",
    "    Useful for creating sample datasets for pandas demonstrations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Predefined lists of realistic names and data\n",
    "    first_names = ['John', 'Jane', 'Michael', 'Emily', 'David', 'Sarah', \n",
    "                   'Robert', 'Jessica', 'James', 'Jennifer', 'Christopher']\n",
    "    last_names = ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', \n",
    "                  'Miller', 'Davis', 'Rodriguez', 'Martinez']\n",
    "    cities = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', \n",
    "              'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', 'Austin']\n",
    "    words = ['product', 'item', 'device', 'tool', 'tech', 'gear', 'pro']\n",
    "    \n",
    "    @staticmethod\n",
    "    def name():\n",
    "        \"\"\"Generate a random full name\"\"\"\n",
    "        return f\"{random.choice(SimpleDataGenerator.first_names)} {random.choice(SimpleDataGenerator.last_names)}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def email():\n",
    "        \"\"\"Generate a random email address\"\"\"\n",
    "        name = ''.join(random.choices(string.ascii_lowercase, k=8))\n",
    "        domain = random.choice(['gmail.com', 'yahoo.com', 'outlook.com', 'email.com'])\n",
    "        return f\"{name}@{domain}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def phone():\n",
    "        \"\"\"Generate a random phone number\"\"\"\n",
    "        return f\"+1-{random.randint(200, 999)}-{random.randint(200, 999)}-{random.randint(1000, 9999)}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def city():\n",
    "        \"\"\"Get a random city\"\"\"\n",
    "        return random.choice(SimpleDataGenerator.cities)\n",
    "    \n",
    "    @staticmethod\n",
    "    def word():\n",
    "        \"\"\"Get a random word for product names\"\"\"\n",
    "        return random.choice(SimpleDataGenerator.words)\n",
    "    \n",
    "    @staticmethod\n",
    "    def seed(val):\n",
    "        \"\"\"Set seed for reproducibility\"\"\"\n",
    "        random.seed(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4a268620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating datasets...\n",
      "\n",
      "================================================================================\n",
      "DATASET GENERATION WITH REALISTIC PATTERNS\n",
      "================================================================================\n",
      "\n",
      "1. Generating 500 customer records...\n",
      "   ✓ Generated 500 customers\n",
      "\n",
      "2. Generating 100 product records...\n",
      "   ✓ Generated 100 products\n",
      "\n",
      "3. Generating 10,000 transaction records...\n",
      "   ✓ Generated 30170 transactions\n",
      "\n",
      "4. Adding realistic data quality issues...\n",
      "   ✓ Added missing values, anomalies, and duplicates\n",
      "\n",
      "5. Generating customer status updates...\n",
      "   ✓ Generated 1464 status updates\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATASET SUMMARY\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Customers: 500 records, 10 columns\n",
      "Products: 100 records, 9 columns\n",
      "Transactions: 30320 records, 11 columns\n",
      "Status Updates: 1464 records, 5 columns\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATASET GENERATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_datasets(num_customers=1000, num_products=200, num_transactions=50000):\n",
    "    \"\"\"\n",
    "    Generate synthetic datasets for the assignment.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    num_customers : int\n",
    "        Number of customer records to generate (default: 1000)\n",
    "    num_products : int\n",
    "        Number of product records to generate (default: 200)\n",
    "    num_transactions : int\n",
    "        Number of transaction records to generate (default: 50000)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (customers_df, products_df, transactions_df, status_updates_df)\n",
    "        Four DataFrames containing the generated data\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    - Includes realistic missing values (~1% per column)\n",
    "    - Includes duplicate transactions (~0.5%)\n",
    "    - Includes negative/invalid amounts for outlier handling\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"DATASET GENERATION WITH REALISTIC PATTERNS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Set seeds for reproducibility - CRITICAL for consistent results\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    SimpleDataGenerator.seed(42)\n",
    "    fake = SimpleDataGenerator()\n",
    "    \n",
    "    # Define date range\n",
    "    start_date = datetime.datetime(2020, 1, 1)\n",
    "    end_date = datetime.datetime(2023, 12, 31)\n",
    "    days_range = (end_date - start_date).days\n",
    "    \n",
    "    # Define business categories\n",
    "    categories = ['Electronics', 'Clothing', 'Home', 'Food', 'Beauty']\n",
    "    subcategories = {\n",
    "        'Electronics': ['Phones', 'Computers', 'Accessories', 'TVs', 'Audio'],\n",
    "        'Clothing': ['Men', 'Women', 'Children', 'Shoes', 'Accessories'],\n",
    "        'Home': ['Furniture', 'Kitchen', 'Decor', 'Bedding', 'Bath'],\n",
    "        'Food': ['Produce', 'Bakery', 'Dairy', 'Meat', 'Beverages'],\n",
    "        'Beauty': ['Skincare', 'Makeup', 'Haircare', 'Fragrance', 'Bath & Body']\n",
    "    }\n",
    "    \n",
    "    # Geographic regions\n",
    "    regions = ['North America', 'Europe', 'Asia', 'South America', 'Africa', 'Oceania']\n",
    "    countries_by_region = {\n",
    "        'North America': ['USA', 'Canada', 'Mexico'],\n",
    "        'Europe': ['UK', 'Germany', 'France', 'Italy', 'Spain'],\n",
    "        'Asia': ['China', 'Japan', 'India', 'South Korea', 'Singapore'],\n",
    "        'South America': ['Brazil', 'Argentina', 'Colombia', 'Chile', 'Peru'],\n",
    "        'Africa': ['South Africa', 'Egypt', 'Nigeria', 'Kenya', 'Morocco'],\n",
    "        'Oceania': ['Australia', 'New Zealand', 'Fiji']\n",
    "    }\n",
    "    \n",
    "    # ========== GENERATE CUSTOMER DATA ==========\n",
    "    print(f\"\\n1. Generating {num_customers:,} customer records...\")\n",
    "    customer_ids = [str(uuid.uuid4()) for _ in range(num_customers)]\n",
    "    customer_data = []\n",
    "    \n",
    "    for customer_id in customer_ids:\n",
    "        # Randomly select region and corresponding country\n",
    "        region = np.random.choice(regions)\n",
    "        country = np.random.choice(countries_by_region[region])\n",
    "        join_date = start_date + datetime.timedelta(days=np.random.randint(0, days_range))\n",
    "        \n",
    "        customer_data.append({\n",
    "            'customer_id': customer_id,\n",
    "            'name': fake.name(),\n",
    "            'email': fake.email(),\n",
    "            'phone': fake.phone(),\n",
    "            'region': region,\n",
    "            'country': country,\n",
    "            'city': fake.city(),\n",
    "            'join_date': join_date,\n",
    "            # Customer tier with realistic distribution\n",
    "            'tier': np.random.choice(['Bronze', 'Silver', 'Gold', 'Platinum'], \n",
    "                                    p=[0.5, 0.3, 0.15, 0.05]),\n",
    "            'is_active': np.random.choice([True, False], p=[0.9, 0.1])\n",
    "        })\n",
    "    \n",
    "    customers_df = pd.DataFrame(customer_data)\n",
    "    print(f\"   ✓ Generated {len(customers_df)} customers\")\n",
    "    \n",
    "    # ========== GENERATE PRODUCT DATA ==========\n",
    "    print(f\"\\n2. Generating {num_products:,} product records...\")\n",
    "    product_ids = [str(uuid.uuid4()) for _ in range(num_products)]\n",
    "    product_data = []\n",
    "    \n",
    "    for product_id in product_ids:\n",
    "        category = np.random.choice(categories)\n",
    "        subcategory = np.random.choice(subcategories[category])\n",
    "        launch_date = start_date + datetime.timedelta(days=np.random.randint(0, days_range))\n",
    "        \n",
    "        product_data.append({\n",
    "            'product_id': product_id,\n",
    "            'name': fake.word() + ' ' + fake.word().capitalize(),\n",
    "            'category': category,\n",
    "            'subcategory': subcategory,\n",
    "            'price': round(np.random.uniform(10, 1000), 2),\n",
    "            'cost': round(np.random.uniform(5, 500), 2),\n",
    "            'weight_kg': round(np.random.uniform(0.1, 20), 2),\n",
    "            'launch_date': launch_date,\n",
    "            'is_discontinued': np.random.choice([True, False], p=[0.1, 0.9])\n",
    "        })\n",
    "    \n",
    "    products_df = pd.DataFrame(product_data)\n",
    "    print(f\"   ✓ Generated {len(products_df)} products\")\n",
    "    \n",
    "    # ========== GENERATE TRANSACTION DATA ==========\n",
    "    print(f\"\\n3. Generating {num_transactions:,} transaction records...\")\n",
    "    transaction_data = []\n",
    "    \n",
    "    for _ in range(num_transactions):\n",
    "        transaction_date = start_date + datetime.timedelta(days=np.random.randint(0, days_range))\n",
    "        customer_id = np.random.choice(customer_ids)\n",
    "        # Each transaction can have multiple items (1-5)\n",
    "        num_items = np.random.randint(1, 6)\n",
    "        \n",
    "        for _ in range(num_items):\n",
    "            product_id = np.random.choice(product_ids)\n",
    "            # Get price from products dataframe\n",
    "            product_price = products_df.loc[products_df['product_id'] == product_id, 'price'].iloc[0]\n",
    "            quantity = np.random.randint(1, 5)\n",
    "            \n",
    "            # Apply realistic discount distribution\n",
    "            discount_pct = np.random.choice([0, 0, 0, 0.05, 0.1, 0.15, 0.2], \n",
    "                                           p=[0.6, 0.1, 0.1, 0.05, 0.05, 0.05, 0.05])\n",
    "            price_after_discount = round(product_price * (1 - discount_pct), 2)\n",
    "            \n",
    "            transaction_id = str(uuid.uuid4())\n",
    "            \n",
    "            transaction_data.append({\n",
    "                'transaction_id': transaction_id,\n",
    "                'customer_id': customer_id,\n",
    "                'product_id': product_id,\n",
    "                'date': transaction_date,\n",
    "                'quantity': quantity,\n",
    "                'unit_price': product_price,\n",
    "                'discount_pct': discount_pct,\n",
    "                'price_after_discount': price_after_discount,\n",
    "                'total_amount': round(quantity * price_after_discount, 2),\n",
    "                'payment_method': np.random.choice(['Credit Card', 'Debit Card', 'PayPal', 'Cash', 'Bank Transfer']),\n",
    "                'store_id': np.random.randint(1, 50)\n",
    "            })\n",
    "    \n",
    "    transactions_df = pd.DataFrame(transaction_data)\n",
    "    print(f\"   ✓ Generated {len(transactions_df)} transactions\")\n",
    "    \n",
    "    # ========== ADD REALISTIC DATA QUALITY ISSUES ==========\n",
    "    print(f\"\\n4. Adding realistic data quality issues...\")\n",
    "    \n",
    "    # Add missing values (~1%)\n",
    "    transactions_df.loc[np.random.choice(transactions_df.index, size=int(len(transactions_df)*0.01)), 'unit_price'] = np.nan\n",
    "    transactions_df.loc[np.random.choice(transactions_df.index, size=int(len(transactions_df)*0.01)), 'discount_pct'] = np.nan\n",
    "    \n",
    "    # Add anomalies (negative amounts)\n",
    "    transactions_df.loc[np.random.choice(transactions_df.index, size=int(len(transactions_df)*0.005)), 'total_amount'] = -1\n",
    "    customers_df.loc[np.random.choice(customers_df.index, size=int(len(customers_df)*0.02)), 'email'] = np.nan\n",
    "    products_df.loc[np.random.choice(products_df.index, size=int(len(products_df)*0.01)), 'price'] = np.nan\n",
    "    \n",
    "    # Add duplicates (~0.5%)\n",
    "    dupe_indices = np.random.choice(transactions_df.index, size=int(len(transactions_df)*0.005))\n",
    "    dupes = transactions_df.loc[dupe_indices].copy()\n",
    "    transactions_df = pd.concat([transactions_df, dupes], ignore_index=True)\n",
    "    \n",
    "    print(f\"   ✓ Added missing values, anomalies, and duplicates\")\n",
    "    \n",
    "    # ========== GENERATE STATUS UPDATES ==========\n",
    "    print(f\"\\n5. Generating customer status updates...\")\n",
    "    status_updates = []\n",
    "    for customer_id in customer_ids:\n",
    "        # Generate 1-5 status updates per customer\n",
    "        num_updates = np.random.randint(1, 6)\n",
    "        for _ in range(num_updates):\n",
    "            update_date = start_date + datetime.timedelta(days=np.random.randint(0, days_range))\n",
    "            status_updates.append({\n",
    "                'customer_id': customer_id,\n",
    "                'update_date': update_date,\n",
    "                'tier': np.random.choice(['Bronze', 'Silver', 'Gold', 'Platinum']),\n",
    "                'lifetime_value': round(np.random.uniform(0, 10000), 2),\n",
    "                'credit_score': np.random.randint(300, 851)\n",
    "            })\n",
    "    \n",
    "    status_updates_df = pd.DataFrame(status_updates)\n",
    "    status_updates_df.sort_values('update_date', inplace=True)\n",
    "    print(f\"   ✓ Generated {len(status_updates_df)} status updates\")\n",
    "    \n",
    "    # ========== SUMMARY ==========\n",
    "    print(f\"\\n\" + \"-\"*80)\n",
    "    print(\"DATASET SUMMARY\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"\\nCustomers: {len(customers_df)} records, {customers_df.shape[1]} columns\")\n",
    "    print(f\"Products: {len(products_df)} records, {products_df.shape[1]} columns\")\n",
    "    print(f\"Transactions: {len(transactions_df)} records, {transactions_df.shape[1]} columns\")\n",
    "    print(f\"Status Updates: {len(status_updates_df)} records, {status_updates_df.shape[1]} columns\")\n",
    "    \n",
    "    return customers_df, products_df, transactions_df, status_updates_df\n",
    "\n",
    "\n",
    "# Generate datasets\n",
    "print(\"\\nGenerating datasets...\\n\")\n",
    "customers_df, products_df, transactions_df, status_updates_df = generate_datasets(\n",
    "    num_customers=500,      # Reduced for faster execution\n",
    "    num_products=100,\n",
    "    num_transactions=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c3299d",
   "metadata": {},
   "source": [
    "## 2. TASK 1: Advanced Data Transformation & Reshaping\n",
    "\n",
    "### Topics Covered:\n",
    "1. **Pivot/Melt Operations** - Converting between long and wide formats\n",
    "2. **Multi-level Indexing** - Creating hierarchical indices\n",
    "3. **Advanced GroupBy** - Using transform, agg, and custom functions\n",
    "\n",
    "### Best Practices:\n",
    "- Use `pivot_table()` for flexible reshaping\n",
    "- Use `melt()` to unpivot data\n",
    "- Use `groupby().transform()` for broadcasting aggregations back to original shape\n",
    "- Use `.xs()` for cross-section extraction from multi-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "10c668ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "█ TASK 1: ADVANCED DATA TRANSFORMATION AND RESHAPING\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "================================================================================\n",
      "1.1: PIVOT AND MELT OPERATIONS\n",
      "================================================================================\n",
      "\n",
      "1. PIVOT: Converting to Wide Format\n",
      "----------------------------------------\n",
      "Pivot Shape: (48, 5)\n",
      "\n",
      "First 5 rows:\n",
      "category      Beauty  Clothing  Electronics      Food      Home\n",
      "year_month                                                     \n",
      "2020-01    191416.13  79410.26    207781.54 216662.44 123052.83\n",
      "2020-02    143038.66  70213.56    147232.19 176362.88 106198.56\n",
      "2020-03    160557.49  96129.90    202878.24 218191.37 148982.16\n",
      "2020-04    168171.00  78350.09    185055.15 238661.47 125440.87\n",
      "2020-05    135484.29  55955.21    180298.41 257277.22 124748.90\n",
      "\n",
      "2. MELT: Converting back to Long Format\n",
      "----------------------------------------\n",
      "Melted Shape: (240, 3)\n",
      "\n",
      "First 10 rows:\n",
      "  year_month category  total_sales\n",
      "0    2020-01   Beauty    191416.13\n",
      "1    2020-02   Beauty    143038.66\n",
      "2    2020-03   Beauty    160557.49\n",
      "3    2020-04   Beauty    168171.00\n",
      "4    2020-05   Beauty    135484.29\n",
      "5    2020-06   Beauty    145128.98\n",
      "6    2020-07   Beauty    144728.87\n",
      "7    2020-08   Beauty    151329.94\n",
      "8    2020-09   Beauty    139689.57\n",
      "9    2020-10   Beauty    178043.09\n",
      "\n",
      "3. PIVOT WITH MARGINS: Adding Row and Column Totals\n",
      "----------------------------------------\n",
      "Shape with Margins: (6, 13)\n",
      "\n",
      "Pivot with Totals:\n",
      "month                1          2          3          4          5          6  \\\n",
      "category                                                                        \n",
      "Beauty       727086.86  588806.33  622831.22  595504.72  619663.57  572224.98   \n",
      "Clothing     355335.25  254826.67  318546.52  297911.94  280155.84  274913.44   \n",
      "Electronics  787813.72  595046.35  712961.80  702854.09  680910.86  721982.45   \n",
      "Food        1003538.22  713194.08  912943.55  919655.91  913367.39  879493.53   \n",
      "Home         526983.13  439438.28  561256.62  478244.45  503727.52  552784.28   \n",
      "Total       3400757.18 2591311.71 3128539.71 2994171.11 2997825.18 3001398.68   \n",
      "\n",
      "month                7          8          9         10         11         12  \\\n",
      "category                                                                        \n",
      "Beauty       616495.77  669465.41  621816.85  634285.17  535102.91  611544.38   \n",
      "Clothing     319012.32  312857.81  338472.21  294711.68  297953.21  307069.73   \n",
      "Electronics  704228.91  709688.71  735553.54  654662.27  747290.69  659833.03   \n",
      "Food         808974.56  828120.70  965019.74  782183.06  878235.76  859710.99   \n",
      "Home         527135.34  567532.96  510840.47  484236.83  509530.10  518097.49   \n",
      "Total       2975846.90 3087665.59 3171702.81 2850079.01 2968112.67 2956255.62   \n",
      "\n",
      "month             Total  \n",
      "category                 \n",
      "Beauty       7414828.17  \n",
      "Clothing     3651766.62  \n",
      "Electronics  8412826.42  \n",
      "Food        10464437.49  \n",
      "Home         6179807.47  \n",
      "Total       36123666.17  \n",
      "\n",
      "================================================================================\n",
      "1.2: MULTI-LEVEL INDEXING\n",
      "================================================================================\n",
      "\n",
      "1. Creating Hierarchical Index\n",
      "----------------------------------------\n",
      "Shape: (30320, 11)\n",
      "Index Levels: ['region', 'category', 'date_only']\n",
      "Number of Levels: 3\n",
      "\n",
      "Sample data:\n",
      "                                                  transaction_id  \\\n",
      "region category date_only                                          \n",
      "Africa Beauty   2020-01-08  6f218611-1410-4ce9-bd2f-d9e6ebfa1d57   \n",
      "                2020-01-08  f56d7acc-52ca-4b8f-bf63-46ba3aab21ce   \n",
      "                2020-01-09  e77b1b31-08b7-4176-b64f-239307c3c36b   \n",
      "                2020-01-10  f5c945cf-107b-470a-b5fb-806b67b5b87b   \n",
      "                2020-01-11  44168888-5266-4350-8691-a973810c82c5   \n",
      "                2020-01-11  4fa932b8-d22f-4738-91dc-17da48a1344e   \n",
      "                2020-01-16  5d6f609e-c8b4-4704-9d06-f861b22f4901   \n",
      "                2020-01-19  900bdb0b-6a13-4c89-aa1c-df1e08527967   \n",
      "                2020-01-19  6bceef77-6a48-4394-a64c-802b231e9b32   \n",
      "                2020-01-19  d00cb4aa-8eb5-4534-a517-60168547968e   \n",
      "\n",
      "                                                     customer_id  \\\n",
      "region category date_only                                          \n",
      "Africa Beauty   2020-01-08  ecbd1440-f1cc-43e4-b651-3e47946aadc0   \n",
      "                2020-01-08  3e394c17-23aa-4b0b-a77e-e5b9978c4641   \n",
      "                2020-01-09  3ed54c49-33b6-4b02-a029-1950f5a861a1   \n",
      "                2020-01-10  501bf055-9571-4d47-aec2-6fb4546f2ac3   \n",
      "                2020-01-11  12be1bdb-1624-4111-ae17-69c75150efc0   \n",
      "                2020-01-11  12be1bdb-1624-4111-ae17-69c75150efc0   \n",
      "                2020-01-16  fe579201-84cb-4b58-ab65-bb80edc2fa8f   \n",
      "                2020-01-19  39c2d94d-1bd5-4ca2-af11-d9ace5f534fa   \n",
      "                2020-01-19  39c2d94d-1bd5-4ca2-af11-d9ace5f534fa   \n",
      "                2020-01-19  39c2d94d-1bd5-4ca2-af11-d9ace5f534fa   \n",
      "\n",
      "                                                      product_id       date  \\\n",
      "region category date_only                                                     \n",
      "Africa Beauty   2020-01-08  a4498220-263a-4c83-9fe8-8fdb3c9ba769 2020-01-08   \n",
      "                2020-01-08  5c020ac9-922c-44dd-8d91-26fa8166627d 2020-01-08   \n",
      "                2020-01-09  b0004874-2550-4766-b139-8180dd014abd 2020-01-09   \n",
      "                2020-01-10  77e0db49-1012-4f77-afa1-edc6d815a537 2020-01-10   \n",
      "                2020-01-11  689eb39a-67cb-43b5-b58f-e5db72bf3af6 2020-01-11   \n",
      "                2020-01-11  050cc12a-44d2-4e09-9878-2a4ddfb4c959 2020-01-11   \n",
      "                2020-01-16  2f903d98-8678-47dd-9f23-29b4937a43d2 2020-01-16   \n",
      "                2020-01-19  77e0db49-1012-4f77-afa1-edc6d815a537 2020-01-19   \n",
      "                2020-01-19  0c49f966-c7e2-49c6-9306-4e68226b0260 2020-01-19   \n",
      "                2020-01-19  14be3df2-f221-4afe-92a1-2fb815c9373b 2020-01-19   \n",
      "\n",
      "                            quantity  unit_price  discount_pct  \\\n",
      "region category date_only                                        \n",
      "Africa Beauty   2020-01-08         4      143.09          0.15   \n",
      "                2020-01-08         4      400.79          0.00   \n",
      "                2020-01-09         1      869.87           NaN   \n",
      "                2020-01-10         3      597.55          0.00   \n",
      "                2020-01-11         3      264.13          0.00   \n",
      "                2020-01-11         2      532.13          0.00   \n",
      "                2020-01-16         2       91.89          0.00   \n",
      "                2020-01-19         3      597.55          0.00   \n",
      "                2020-01-19         2      705.61          0.10   \n",
      "                2020-01-19         4      553.49          0.00   \n",
      "\n",
      "                            price_after_discount  total_amount payment_method  \\\n",
      "region category date_only                                                       \n",
      "Africa Beauty   2020-01-08                121.63        486.52    Credit Card   \n",
      "                2020-01-08                400.79       1603.16           Cash   \n",
      "                2020-01-09                869.87        869.87     Debit Card   \n",
      "                2020-01-10                597.55       1792.65    Credit Card   \n",
      "                2020-01-11                264.13        792.39  Bank Transfer   \n",
      "                2020-01-11                532.13       1064.26         PayPal   \n",
      "                2020-01-16                 91.89        183.78     Debit Card   \n",
      "                2020-01-19                597.55       1792.65         PayPal   \n",
      "                2020-01-19                635.05       1270.10     Debit Card   \n",
      "                2020-01-19                553.49       2213.96  Bank Transfer   \n",
      "\n",
      "                            store_id  \n",
      "region category date_only             \n",
      "Africa Beauty   2020-01-08        12  \n",
      "                2020-01-08         7  \n",
      "                2020-01-09        17  \n",
      "                2020-01-10        23  \n",
      "                2020-01-11        22  \n",
      "                2020-01-11        17  \n",
      "                2020-01-16        20  \n",
      "                2020-01-19        28  \n",
      "                2020-01-19        21  \n",
      "                2020-01-19        49  \n",
      "\n",
      "2. Cross-Section (xs) Operations\n",
      "----------------------------------------\n",
      "\n",
      "Data for Africa region: 5338 rows\n",
      "                                           transaction_id  \\\n",
      "category date_only                                          \n",
      "Beauty   2020-01-08  6f218611-1410-4ce9-bd2f-d9e6ebfa1d57   \n",
      "         2020-01-08  f56d7acc-52ca-4b8f-bf63-46ba3aab21ce   \n",
      "         2020-01-09  e77b1b31-08b7-4176-b64f-239307c3c36b   \n",
      "         2020-01-10  f5c945cf-107b-470a-b5fb-806b67b5b87b   \n",
      "         2020-01-11  44168888-5266-4350-8691-a973810c82c5   \n",
      "\n",
      "                                              customer_id  \\\n",
      "category date_only                                          \n",
      "Beauty   2020-01-08  ecbd1440-f1cc-43e4-b651-3e47946aadc0   \n",
      "         2020-01-08  3e394c17-23aa-4b0b-a77e-e5b9978c4641   \n",
      "         2020-01-09  3ed54c49-33b6-4b02-a029-1950f5a861a1   \n",
      "         2020-01-10  501bf055-9571-4d47-aec2-6fb4546f2ac3   \n",
      "         2020-01-11  12be1bdb-1624-4111-ae17-69c75150efc0   \n",
      "\n",
      "                                               product_id       date  \\\n",
      "category date_only                                                     \n",
      "Beauty   2020-01-08  a4498220-263a-4c83-9fe8-8fdb3c9ba769 2020-01-08   \n",
      "         2020-01-08  5c020ac9-922c-44dd-8d91-26fa8166627d 2020-01-08   \n",
      "         2020-01-09  b0004874-2550-4766-b139-8180dd014abd 2020-01-09   \n",
      "         2020-01-10  77e0db49-1012-4f77-afa1-edc6d815a537 2020-01-10   \n",
      "         2020-01-11  689eb39a-67cb-43b5-b58f-e5db72bf3af6 2020-01-11   \n",
      "\n",
      "                     quantity  unit_price  discount_pct  price_after_discount  \\\n",
      "category date_only                                                              \n",
      "Beauty   2020-01-08         4      143.09          0.15                121.63   \n",
      "         2020-01-08         4      400.79          0.00                400.79   \n",
      "         2020-01-09         1      869.87           NaN                869.87   \n",
      "         2020-01-10         3      597.55          0.00                597.55   \n",
      "         2020-01-11         3      264.13          0.00                264.13   \n",
      "\n",
      "                     total_amount payment_method  store_id  \n",
      "category date_only                                          \n",
      "Beauty   2020-01-08        486.52    Credit Card        12  \n",
      "         2020-01-08       1603.16           Cash         7  \n",
      "         2020-01-09        869.87     Debit Card        17  \n",
      "         2020-01-10       1792.65    Credit Card        23  \n",
      "         2020-01-11        792.39  Bank Transfer        22  \n",
      "\n",
      "3. Unstack and Restack Operations\n",
      "----------------------------------------\n",
      "\n",
      "Unstacked Shape: (6926, 6)\n",
      "(Regions now as columns)\n",
      "region               Africa    Asia  Europe  North America  Oceania  \\\n",
      "category date_only                                                    \n",
      "Beauty   2020-01-02    0.00  997.11  661.22         349.20     0.00   \n",
      "         2020-01-03    0.00    0.00    0.00           0.00  2142.36   \n",
      "         2020-01-04    0.00    0.00    0.00        2921.97     0.00   \n",
      "         2020-01-05    0.00    0.00  316.71           0.00     0.00   \n",
      "         2020-01-06    0.00 2433.27    0.00           0.00     0.00   \n",
      "\n",
      "region               South America  \n",
      "category date_only                  \n",
      "Beauty   2020-01-02         811.09  \n",
      "         2020-01-03         932.73  \n",
      "         2020-01-04           0.00  \n",
      "         2020-01-05           0.00  \n",
      "         2020-01-06        1614.04  \n",
      "\n",
      "Restacked Shape: (41556,)\n",
      "(Back to hierarchical structure)\n",
      "category  date_only   region       \n",
      "Beauty    2020-01-02  Africa            0.00\n",
      "                      Asia            997.11\n",
      "                      Europe          661.22\n",
      "                      North America   349.20\n",
      "                      Oceania           0.00\n",
      "                      South America   811.09\n",
      "          2020-01-03  Africa            0.00\n",
      "                      Asia              0.00\n",
      "                      Europe            0.00\n",
      "                      North America     0.00\n",
      "dtype: float64\n",
      "\n",
      "================================================================================\n",
      "1.3: ADVANCED GROUPBY OPERATIONS\n",
      "================================================================================\n",
      "\n",
      "1. Using Transform to Normalize Values\n",
      "----------------------------------------\n",
      "Normalized values (z-score per category):\n",
      "      category  total_amount  category_normalized\n",
      "0  Electronics        591.04                -0.70\n",
      "1     Clothing       1025.58                 0.01\n",
      "2       Beauty        705.61                -0.52\n",
      "3     Clothing       3039.64                 2.44\n",
      "4  Electronics       2130.06                 1.03\n",
      "5         Home       1855.20                 0.67\n",
      "6  Electronics       2304.36                 1.22\n",
      "7     Clothing         47.60                -1.17\n",
      "8       Beauty       2390.20                 1.38\n",
      "9         Home       1535.94                 0.35\n",
      "\n",
      "2. Applying Multiple Aggregation Functions\n",
      "----------------------------------------\n",
      "\n",
      "Aggregation Result Shape: (5, 9)\n",
      "            total_amount                                    quantity         \\\n",
      "                     sum    mean    std   min     max count     mean median   \n",
      "category                                                                      \n",
      "Beauty        7414828.17 1168.24 884.22 -1.00 3988.44  6347     2.51   3.00   \n",
      "Clothing      3651766.62 1017.77 827.36 -1.00 3039.64  3588     2.52   3.00   \n",
      "Electronics   8412826.42 1212.92 891.33 -1.00 3822.04  6936     2.48   2.00   \n",
      "Food         10464437.49 1267.49 910.28 -1.00 3778.16  8256     2.49   2.00   \n",
      "Home          6179807.47 1190.03 996.33 -1.00 3962.56  5193     2.49   3.00   \n",
      "\n",
      "            discount_pct  \n",
      "                    mean  \n",
      "category                  \n",
      "Beauty              0.02  \n",
      "Clothing            0.03  \n",
      "Electronics         0.03  \n",
      "Food                0.02  \n",
      "Home                0.03  \n",
      "\n",
      "3. Custom Aggregation Function\n",
      "----------------------------------------\n",
      "\n",
      "Custom Aggregation Results:\n",
      "             Total Revenue  Mean Sale  Var-to-Mean Ratio\n",
      "category                                                \n",
      "Beauty          7414828.17    1168.24             669.25\n",
      "Clothing        3651766.62    1017.77             672.57\n",
      "Electronics     8412826.42    1212.92             655.01\n",
      "Food           10464437.49    1267.49             653.74\n",
      "Home            6179807.47    1190.03             834.16\n",
      "\n",
      "4. Filtering Groups by Criteria\n",
      "----------------------------------------\n",
      "\n",
      "Categories with sales > median ($7,414,828.17):\n",
      "  Filtered records: 15192\n",
      "  Categories: [np.str_('Electronics'), np.str_('Food')]\n",
      "\n",
      "✓ Task 1 Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TASK 1: ADVANCED DATA TRANSFORMATION AND RESHAPING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"█\"*80)\n",
    "print(\"█ TASK 1: ADVANCED DATA TRANSFORMATION AND RESHAPING\")\n",
    "print(\"█\"*80)\n",
    "\n",
    "class Task1_DataTransformation:\n",
    "    \"\"\"\n",
    "    Demonstrates advanced pandas data transformation techniques.\n",
    "    \n",
    "    Methods:\n",
    "    - pivot_and_melt_operations: Convert between long/wide formats\n",
    "    - multilevel_indexing: Create and manipulate hierarchical indices\n",
    "    - advanced_groupby_operations: Advanced aggregation and filtering\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, transactions_df, products_df, customers_df):\n",
    "        \"\"\"\n",
    "        Initialize Task 1 with data references.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        transactions_df : DataFrame\n",
    "            Transaction records\n",
    "        products_df : DataFrame\n",
    "            Product master data\n",
    "        customers_df : DataFrame\n",
    "            Customer master data\n",
    "        \"\"\"\n",
    "        # Create copies to avoid modifying original data\n",
    "        self.transactions = transactions_df.copy()\n",
    "        self.products = products_df.copy()\n",
    "        self.customers = customers_df.copy()\n",
    "    \n",
    "    def pivot_and_melt_operations(self):\n",
    "        \"\"\"\n",
    "        Demonstrate pivot (long to wide) and melt (wide to long) operations.\n",
    "        \n",
    "        Key Concepts:\n",
    "        - pivot_table(): More flexible, allows aggregation\n",
    "        - melt(): Convert wide format back to long\n",
    "        - margins: Add row/column totals\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"1.1: PIVOT AND MELT OPERATIONS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Merge to get category information\n",
    "        df = self.transactions.merge(\n",
    "            self.products[['product_id', 'category']], \n",
    "            on='product_id', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Create year-month period for analysis\n",
    "        df['year_month'] = df['date'].dt.to_period('M')\n",
    "        \n",
    "        # ===== PIVOT: Long to Wide Format =====\n",
    "        print(\"\\n1. PIVOT: Converting to Wide Format\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Create pivot table with monthly sales by category\n",
    "        pivot_wide = df.pivot_table(\n",
    "            index='year_month',           # Rows\n",
    "            columns='category',           # Columns\n",
    "            values='total_amount',        # Values to aggregate\n",
    "            aggfunc='sum',                # Aggregation function\n",
    "            fill_value=0                  # Fill missing values with 0\n",
    "        )\n",
    "        \n",
    "        print(f\"Pivot Shape: {pivot_wide.shape}\")\n",
    "        print(\"\\nFirst 5 rows:\")\n",
    "        print(pivot_wide.head())\n",
    "        \n",
    "        # ===== MELT: Wide to Long Format =====\n",
    "        print(\"\\n2. MELT: Converting back to Long Format\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Reset index to make year_month a column\n",
    "        pivot_reset = pivot_wide.reset_index()\n",
    "        \n",
    "        # Melt back to long format\n",
    "        melted = pivot_reset.melt(\n",
    "            id_vars=['year_month'],         # Columns to keep as-is\n",
    "            var_name='category',            # Name for pivoted columns\n",
    "            value_name='total_sales'        # Name for values\n",
    "        )\n",
    "        \n",
    "        print(f\"Melted Shape: {melted.shape}\")\n",
    "        print(\"\\nFirst 10 rows:\")\n",
    "        print(melted.head(10))\n",
    "        \n",
    "        # ===== PIVOT WITH MARGINS (Totals) =====\n",
    "        print(\"\\n3. PIVOT WITH MARGINS: Adding Row and Column Totals\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Add month column for granular analysis\n",
    "        df_temp = df.copy()\n",
    "        df_temp['month'] = df_temp['date'].dt.month\n",
    "        \n",
    "        # Pivot with margins=True adds totals\n",
    "        pivot_with_totals = df_temp.pivot_table(\n",
    "            index='category',\n",
    "            columns='month',\n",
    "            values='total_amount',\n",
    "            aggfunc='sum',\n",
    "            margins=True,                   # Add totals\n",
    "            margins_name='Total'\n",
    "        )\n",
    "        \n",
    "        print(f\"Shape with Margins: {pivot_with_totals.shape}\")\n",
    "        print(\"\\nPivot with Totals:\")\n",
    "        print(pivot_with_totals)\n",
    "        \n",
    "        return pivot_wide, melted, pivot_with_totals\n",
    "    \n",
    "    def multilevel_indexing(self):\n",
    "        \"\"\"\n",
    "        Demonstrate hierarchical (multi-level) indexing.\n",
    "        \n",
    "        Key Concepts:\n",
    "        - set_index(): Create multi-level index\n",
    "        - xs(): Cross-section extraction\n",
    "        - unstack()/stack(): Reshape hierarchical data\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"1.2: MULTI-LEVEL INDEXING\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Merge necessary data\n",
    "        df = self.transactions.merge(\n",
    "            self.products[['product_id', 'category']], \n",
    "            on='product_id', \n",
    "            how='left'\n",
    "        ).merge(\n",
    "            self.customers[['customer_id', 'region']], \n",
    "            on='customer_id', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Create date column for indexing\n",
    "        df['date_only'] = df['date'].dt.date\n",
    "        \n",
    "        # ===== CREATE HIERARCHICAL INDEX =====\n",
    "        print(\"\\n1. Creating Hierarchical Index\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Create 3-level index: region > category > date\n",
    "        hierarchical = df.set_index(['region', 'category', 'date_only']).sort_index()\n",
    "        \n",
    "        print(f\"Shape: {hierarchical.shape}\")\n",
    "        print(f\"Index Levels: {hierarchical.index.names}\")\n",
    "        print(f\"Number of Levels: {hierarchical.index.nlevels}\")\n",
    "        print(\"\\nSample data:\")\n",
    "        print(hierarchical.head(10))\n",
    "        \n",
    "        # ===== CROSS-SECTION (XS) OPERATIONS =====\n",
    "        print(\"\\n2. Cross-Section (xs) Operations\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Extract data for specific region\n",
    "        regions_available = hierarchical.index.get_level_values(0).unique()\n",
    "        first_region = regions_available[0]\n",
    "        \n",
    "        subset = hierarchical.xs(first_region, level='region')\n",
    "        print(f\"\\nData for {first_region} region: {subset.shape[0]} rows\")\n",
    "        print(subset.head())\n",
    "        \n",
    "        # ===== UNSTACK/RESTACK OPERATIONS =====\n",
    "        print(\"\\n3. Unstack and Restack Operations\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # First aggregate to remove duplicates\n",
    "        hierarchical_agg = hierarchical['total_amount'].groupby(level=[0, 1, 2]).sum()\n",
    "        \n",
    "        # Unstack: Convert index level to columns\n",
    "        unstacked = hierarchical_agg.unstack(level=0, fill_value=0)\n",
    "        print(f\"\\nUnstacked Shape: {unstacked.shape}\")\n",
    "        print(\"(Regions now as columns)\")\n",
    "        print(unstacked.head())\n",
    "        \n",
    "        # Restack: Convert columns back to index\n",
    "        restacked = unstacked.stack()\n",
    "        print(f\"\\nRestacked Shape: {restacked.shape}\")\n",
    "        print(\"(Back to hierarchical structure)\")\n",
    "        print(restacked.head(10))\n",
    "        \n",
    "        return hierarchical, unstacked, restacked\n",
    "    \n",
    "    def advanced_groupby_operations(self):\n",
    "        \"\"\"\n",
    "        Demonstrate advanced groupby operations.\n",
    "        \n",
    "        Key Concepts:\n",
    "        - transform(): Apply function to each group and broadcast result\n",
    "        - agg(): Apply multiple aggregation functions\n",
    "        - filter(): Select groups meeting criteria\n",
    "        - Custom aggregation functions\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"1.3: ADVANCED GROUPBY OPERATIONS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        df = self.transactions.merge(\n",
    "            self.products[['product_id', 'category']], \n",
    "            on='product_id', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # ===== TRANSFORM: Group and Broadcast =====\n",
    "        print(\"\\n1. Using Transform to Normalize Values\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Transform normalizes each value relative to its group\n",
    "        df['category_normalized'] = df.groupby('category')['total_amount'].transform(\n",
    "            lambda x: (x - x.mean()) / x.std()  # Z-score normalization\n",
    "        )\n",
    "        \n",
    "        print(\"Normalized values (z-score per category):\")\n",
    "        print(df[['category', 'total_amount', 'category_normalized']].head(10))\n",
    "        \n",
    "        # ===== MULTIPLE AGGREGATIONS =====\n",
    "        print(\"\\n2. Applying Multiple Aggregation Functions\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Apply different functions to different columns\n",
    "        agg_result = df.groupby('category').agg({\n",
    "            'total_amount': ['sum', 'mean', 'std', 'min', 'max', 'count'],\n",
    "            'quantity': ['mean', 'median'],\n",
    "            'discount_pct': 'mean'\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nAggregation Result Shape: {agg_result.shape}\")\n",
    "        print(agg_result)\n",
    "        \n",
    "        # ===== CUSTOM AGGREGATION FUNCTION =====\n",
    "        print(\"\\n3. Custom Aggregation Function\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        def revenue_variance_ratio(x):\n",
    "            \"\"\"Calculate variance to mean ratio (coefficient of variation)\"\"\"\n",
    "            if len(x) > 0 and x.mean() != 0:\n",
    "                return x.var() / x.mean()\n",
    "            return 0\n",
    "        \n",
    "        # Apply custom function\n",
    "        custom_agg = df.groupby('category')['total_amount'].agg([\n",
    "            ('Total Revenue', 'sum'),\n",
    "            ('Mean Sale', 'mean'),\n",
    "            ('Var-to-Mean Ratio', revenue_variance_ratio)\n",
    "        ])\n",
    "        \n",
    "        print(\"\\nCustom Aggregation Results:\")\n",
    "        print(custom_agg)\n",
    "        \n",
    "        # ===== FILTER: Select Groups Meeting Criteria =====\n",
    "        print(\"\\n4. Filtering Groups by Criteria\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Calculate category totals\n",
    "        category_totals = df.groupby('category')['total_amount'].sum()\n",
    "        median_sales = category_totals.median()\n",
    "        \n",
    "        # Filter: Keep only rows from high-sales categories\n",
    "        high_sales = df.groupby('category').filter(\n",
    "            lambda x: x['total_amount'].sum() > median_sales\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nCategories with sales > median (${median_sales:,.2f}):\")\n",
    "        print(f\"  Filtered records: {len(high_sales)}\")\n",
    "        print(f\"  Categories: {high_sales['category'].unique().tolist()}\")\n",
    "        \n",
    "        return agg_result, custom_agg, high_sales\n",
    "\n",
    "\n",
    "# Execute Task 1\n",
    "task1 = Task1_DataTransformation(transactions_df, products_df, customers_df)\n",
    "pivot_wide, melted, pivot_totals = task1.pivot_and_melt_operations()\n",
    "hierarchical, unstacked, restacked = task1.multilevel_indexing()\n",
    "agg_result, custom_agg, high_sales = task1.advanced_groupby_operations()\n",
    "\n",
    "print(\"\\n✓ Task 1 Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8983466",
   "metadata": {},
   "source": [
    "## 3. TASK 2: Advanced Merging & Joining\n",
    "\n",
    "### Topics Covered:\n",
    "1. **Complex Joins** - Three-way joins, self-joins, different join types\n",
    "2. **Duplicate Handling** - Identify and resolve duplicates\n",
    "3. **Time-Based Joins** - AsOf joins, time-window joins\n",
    "\n",
    "### Best Practices:\n",
    "- Understand join semantics (inner, left, right, outer)\n",
    "- Check for duplicates before merging\n",
    "- Handle column name conflicts with suffixes\n",
    "- Validate merge integrity post-operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7331f265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "█ TASK 2: ADVANCED MERGING AND JOINING\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "================================================================================\n",
      "2.1: COMPLEX JOINS\n",
      "================================================================================\n",
      "\n",
      "1. Three-Way Join (Transactions + Customers + Products)\n",
      "----------------------------------------\n",
      "Three-way join shape: (30320, 15)\n",
      "Columns: ['transaction_id', 'customer_id', 'product_id', 'date', 'quantity', 'unit_price', 'discount_pct', 'price_after_discount', 'total_amount', 'payment_method', 'store_id', 'region', 'tier', 'category', 'price']...\n",
      "\n",
      "Sample data:\n",
      "                            customer_id         region    tier  \\\n",
      "0  7643eb8a-faab-4ac0-8cfb-4309b1befce2  South America  Bronze   \n",
      "1  7643eb8a-faab-4ac0-8cfb-4309b1befce2  South America  Bronze   \n",
      "2  2b701961-a805-4520-911e-65a5d7505155  North America  Bronze   \n",
      "3  66751b17-40b4-41f2-82f9-008dd3cc8e1e           Asia    Gold   \n",
      "4  66751b17-40b4-41f2-82f9-008dd3cc8e1e           Asia    Gold   \n",
      "5  77abe375-c402-4856-ba5c-5475e0a41ae6         Europe  Silver   \n",
      "6  77abe375-c402-4856-ba5c-5475e0a41ae6         Europe  Silver   \n",
      "7  77abe375-c402-4856-ba5c-5475e0a41ae6         Europe  Silver   \n",
      "8  1eb80fd6-3b89-4952-a1a8-79727c500a62         Europe  Bronze   \n",
      "9  1eb80fd6-3b89-4952-a1a8-79727c500a62         Europe  Bronze   \n",
      "\n",
      "                             product_id     category  \n",
      "0  888bceab-8d98-47a0-8e77-e2f32af5b881  Electronics  \n",
      "1  581aff36-4fcd-4953-b2d5-fd6e2a0ad719     Clothing  \n",
      "2  0c49f966-c7e2-49c6-9306-4e68226b0260       Beauty  \n",
      "3  fec137be-66ca-4c67-8be0-109c10b761b7     Clothing  \n",
      "4  c54b2366-c214-4d64-a273-f2747c7b2c0f  Electronics  \n",
      "5  69862861-3c28-4c4a-bc7f-0639d5d1f7e8         Home  \n",
      "6  47e20c8f-02bd-4960-91b9-974a8bdff10a  Electronics  \n",
      "7  12f9f564-dd73-4958-8819-9eebd8b8ef29     Clothing  \n",
      "8  77e0db49-1012-4f77-afa1-edc6d815a537       Beauty  \n",
      "9  e845f730-ec2f-41b2-84a1-4931eb90fa8d         Home  \n",
      "\n",
      "2. Self-Join (Finding Related Records)\n",
      "----------------------------------------\n",
      "Self-join result shape: (1588, 5)\n",
      "\n",
      "Sample (customers in same region):\n",
      "                       customer_id_cust1  \\\n",
      "1   e21c7a52-1fcb-4848-a614-9d2e53f6b555   \n",
      "2   e21c7a52-1fcb-4848-a614-9d2e53f6b555   \n",
      "3   e21c7a52-1fcb-4848-a614-9d2e53f6b555   \n",
      "4   e21c7a52-1fcb-4848-a614-9d2e53f6b555   \n",
      "5   e21c7a52-1fcb-4848-a614-9d2e53f6b555   \n",
      "6   e21c7a52-1fcb-4848-a614-9d2e53f6b555   \n",
      "7   e21c7a52-1fcb-4848-a614-9d2e53f6b555   \n",
      "8   e21c7a52-1fcb-4848-a614-9d2e53f6b555   \n",
      "9   e21c7a52-1fcb-4848-a614-9d2e53f6b555   \n",
      "10  e21c7a52-1fcb-4848-a614-9d2e53f6b555   \n",
      "\n",
      "                       customer_id_cust2         region  \n",
      "1   b4ffd36d-c146-44a9-bb01-bf9e83d74b1d  South America  \n",
      "2   6bedf402-2a14-4ccd-9254-a3d9892130b7  South America  \n",
      "3   8548ba01-59f2-47fe-9866-f68e14e0210e  South America  \n",
      "4   1e46be5f-ec2a-467a-b422-1a0e0f4ba35a  South America  \n",
      "5   03e7f225-2d5c-4b4e-bd70-f0b820b32158  South America  \n",
      "6   0d35ec76-874c-43a0-9ab5-12347357e1df  South America  \n",
      "7   a35e96f7-53b0-45f1-9808-f696a60acae4  South America  \n",
      "8   c3066eec-1cd3-4306-9b01-d240791f83f3  South America  \n",
      "9   75f919ba-81e9-4c0a-983e-ee8830855fe5  South America  \n",
      "10  7cc606cb-05b1-45c4-bd16-3f9dfb2757cf  South America  \n",
      "\n",
      "3. Comparing Different Join Types\n",
      "----------------------------------------\n",
      "  INNER :   23 rows\n",
      "  LEFT  :   50 rows\n",
      "  RIGHT :   45 rows\n",
      "  OUTER :   72 rows\n",
      "\n",
      "================================================================================\n",
      "2.2: HANDLING DUPLICATES AND CONFLICTS\n",
      "================================================================================\n",
      "\n",
      "1. Identifying Duplicate Records\n",
      "----------------------------------------\n",
      "Duplicate transaction_ids: 300\n",
      "\n",
      "Sample duplicates:\n",
      "                            transaction_id  \\\n",
      "139   1819e105-f9a4-4138-93d4-8ef8ceac534b   \n",
      "608   cd8f9a25-0b14-4b25-9477-8dc8fc89ef16   \n",
      "728   26936746-9b72-46a2-a8a0-667d76390e99   \n",
      "966   b479c52e-6309-417b-9003-bb4d39058e49   \n",
      "1178  dd3b57a0-ab01-4179-80ee-45ccd0481d20   \n",
      "1184  1a4bf4f9-beea-434a-b6ca-74462275baa3   \n",
      "1219  a71171f9-1be8-40ca-b79a-2fa9244eb3a5   \n",
      "1274  7b21c551-3a86-4bba-8b76-b8b53c3ceff0   \n",
      "1359  a533a4c5-4f2a-49f8-9461-6532124b4bbc   \n",
      "1388  68a2cfb4-33e5-4754-97f6-6d9cfc7d9b0d   \n",
      "\n",
      "                               customer_id  total_amount  \n",
      "139   18ed1b8c-26c8-4653-ab23-406f234891e1        710.02  \n",
      "608   d316e18d-34a9-4c1e-97d8-7feb6d6e3154        698.94  \n",
      "728   31169741-35a8-4581-8cdb-beda1790a8ce        666.45  \n",
      "966   8e7ee6ee-9598-43a5-9a49-ed716c9bf401         25.70  \n",
      "1178  fb6b7e66-efab-4d0b-90cc-f0c01fb4cf3d       1870.68  \n",
      "1184  f35557f9-54f9-48f1-b4c1-cc8568d4a763        821.28  \n",
      "1219  0ef0b283-7ade-4b81-b96d-5290ad0a8c24       2047.92  \n",
      "1274  8ba5280e-155b-4c29-9c10-c16784bbf98a         98.30  \n",
      "1359  2463eb6b-da2a-47d7-b1e9-ce57c440b184        176.48  \n",
      "1388  fb6b7e66-efab-4d0b-90cc-f0c01fb4cf3d        105.57  \n",
      "\n",
      "2. Merge Validation Function\n",
      "----------------------------------------\n",
      "\n",
      "Validation Results:\n",
      "  left_rows: 1000\n",
      "  merged_rows: 1000\n",
      "  rows_lost: 0\n",
      "  valid: True\n",
      "\n",
      "3. Handling Column Name Conflicts\n",
      "----------------------------------------\n",
      "Merged shape: (1464, 3)\n",
      "Columns: ['customer_id', 'customer_tier', 'status_tier']\n",
      "\n",
      "Sample data:\n",
      "                            customer_id customer_tier status_tier\n",
      "0  e21c7a52-1fcb-4848-a614-9d2e53f6b555        Silver      Silver\n",
      "1  e21c7a52-1fcb-4848-a614-9d2e53f6b555        Silver      Bronze\n",
      "2  e21c7a52-1fcb-4848-a614-9d2e53f6b555        Silver    Platinum\n",
      "3  40758cef-7491-4a95-b474-096bfa7dc749        Bronze        Gold\n",
      "4  40758cef-7491-4a95-b474-096bfa7dc749        Bronze        Gold\n",
      "5  40758cef-7491-4a95-b474-096bfa7dc749        Bronze      Silver\n",
      "6  40758cef-7491-4a95-b474-096bfa7dc749        Bronze    Platinum\n",
      "7  b4ffd36d-c146-44a9-bb01-bf9e83d74b1d      Platinum      Bronze\n",
      "8  b4ffd36d-c146-44a9-bb01-bf9e83d74b1d      Platinum        Gold\n",
      "9  3fdcdeaf-0693-4c6e-ae84-c3196c23f9ae      Platinum      Silver\n",
      "\n",
      "================================================================================\n",
      "2.3: TIME-BASED JOINS\n",
      "================================================================================\n",
      "\n",
      "1. AsOf Join - Match by Nearest Previous Timestamp\n",
      "----------------------------------------\n",
      "AsOf join result shape: (100, 5)\n",
      "\n",
      "Sample data (transactions with matched status):\n",
      "                            customer_id transaction_date update_date tier\n",
      "0  e69387ef-b636-4c42-b538-0f6c222a2352       2020-01-01         NaT  NaN\n",
      "1  e69387ef-b636-4c42-b538-0f6c222a2352       2020-01-01         NaT  NaN\n",
      "2  e69387ef-b636-4c42-b538-0f6c222a2352       2020-01-01         NaT  NaN\n",
      "3  c19f586b-07d3-4b30-a4fd-5380472fac91       2020-01-01         NaT  NaN\n",
      "4  e69387ef-b636-4c42-b538-0f6c222a2352       2020-01-01         NaT  NaN\n",
      "5  508c927f-1074-4165-869c-11617c182a5b       2020-01-01         NaT  NaN\n",
      "6  508c927f-1074-4165-869c-11617c182a5b       2020-01-01         NaT  NaN\n",
      "7  508c927f-1074-4165-869c-11617c182a5b       2020-01-01         NaT  NaN\n",
      "8  c19f586b-07d3-4b30-a4fd-5380472fac91       2020-01-01         NaT  NaN\n",
      "9  e368e70b-0c93-4708-9fa5-11972da32a10       2020-01-01         NaT  NaN\n",
      "\n",
      "2. Time Window Join - Match Within 30 Days\n",
      "----------------------------------------\n",
      "\n",
      "Window join result shape: (0, 0)\n",
      "\n",
      "Sample data (within 30-day window):\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "✓ Task 2 Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TASK 2: ADVANCED MERGING AND JOINING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"█\"*80)\n",
    "print(\"█ TASK 2: ADVANCED MERGING AND JOINING\")\n",
    "print(\"█\"*80)\n",
    "\n",
    "class Task2_MergingAndJoining:\n",
    "    \"\"\"\n",
    "    Demonstrates advanced pandas joining and merging techniques.\n",
    "    \n",
    "    Methods:\n",
    "    - complex_joins: Multiple joins, self-joins, join types comparison\n",
    "    - handling_duplicates: Identify and resolve duplicate keys\n",
    "    - time_based_joins: AsOf joins and time-window joins\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, transactions_df, products_df, customers_df, status_updates_df):\n",
    "        \"\"\"Initialize with all necessary dataframes\"\"\"\n",
    "        self.transactions = transactions_df.copy()\n",
    "        self.products = products_df.copy()\n",
    "        self.customers = customers_df.copy()\n",
    "        self.status_updates = status_updates_df.copy()\n",
    "    \n",
    "    def complex_joins(self):\n",
    "        \"\"\"\n",
    "        Demonstrate complex join operations.\n",
    "        \n",
    "        Includes:\n",
    "        - Three-way joins combining multiple datasets\n",
    "        - Self-joins for finding relationships\n",
    "        - Comparison of different join types\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"2.1: COMPLEX JOINS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # ===== THREE-WAY JOIN =====\n",
    "        print(\"\\n1. Three-Way Join (Transactions + Customers + Products)\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Chain multiple merges: transactions > customers > products\n",
    "        three_way = (\n",
    "            self.transactions\n",
    "            .merge(self.customers[['customer_id', 'region', 'tier']], \n",
    "                  on='customer_id', how='left')\n",
    "            .merge(self.products[['product_id', 'category', 'price']], \n",
    "                  on='product_id', how='left', suffixes=('_trans', '_prod'))\n",
    "        )\n",
    "        \n",
    "        print(f\"Three-way join shape: {three_way.shape}\")\n",
    "        print(f\"Columns: {list(three_way.columns)[:15]}...\")  # Show first 15\n",
    "        print(\"\\nSample data:\")\n",
    "        print(three_way[['customer_id', 'region', 'tier', 'product_id', 'category']].head(10))\n",
    "        \n",
    "        # ===== SELF-JOIN =====\n",
    "        print(\"\\n2. Self-Join (Finding Related Records)\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Self-join to find customers in same region\n",
    "        customers_subset = self.customers[['customer_id', 'region', 'tier']].head(100)\n",
    "        \n",
    "        self_join = (\n",
    "            customers_subset\n",
    "            .merge(customers_subset, on='region', suffixes=('_cust1', '_cust2'))\n",
    "            .query('customer_id_cust1 != customer_id_cust2')  # Remove same customer\n",
    "        )\n",
    "        \n",
    "        print(f\"Self-join result shape: {self_join.shape}\")\n",
    "        print(\"\\nSample (customers in same region):\")\n",
    "        print(self_join[['customer_id_cust1', 'customer_id_cust2', 'region']].head(10))\n",
    "        \n",
    "        # ===== COMPARE JOIN TYPES =====\n",
    "        print(\"\\n3. Comparing Different Join Types\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Use small subset for demonstration\n",
    "        small_trans = self.transactions.head(50)\n",
    "        small_products = self.products.head(40)\n",
    "        \n",
    "        join_comparison = {}\n",
    "        for how in ['inner', 'left', 'right', 'outer']:\n",
    "            joined = small_trans.merge(\n",
    "                small_products[['product_id', 'category']],\n",
    "                on='product_id',\n",
    "                how=how\n",
    "            )\n",
    "            join_comparison[how] = len(joined)\n",
    "            print(f\"  {how.upper():6s}: {len(joined):4d} rows\")\n",
    "        \n",
    "        return three_way, self_join, join_comparison\n",
    "    \n",
    "    def handling_duplicates_and_conflicts(self):\n",
    "        \"\"\"\n",
    "        Identify and handle duplicates and conflicts in merging.\n",
    "        \n",
    "        Includes:\n",
    "        - Duplicate detection\n",
    "        - Merge validation\n",
    "        - Conflict resolution\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"2.2: HANDLING DUPLICATES AND CONFLICTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # ===== IDENTIFY DUPLICATES =====\n",
    "        print(\"\\n1. Identifying Duplicate Records\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check for duplicate transaction IDs\n",
    "        dup_mask = self.transactions.duplicated(subset=['transaction_id'], keep=False)\n",
    "        num_dups = dup_mask.sum()\n",
    "        \n",
    "        print(f\"Duplicate transaction_ids: {num_dups}\")\n",
    "        if num_dups > 0:\n",
    "            print(\"\\nSample duplicates:\")\n",
    "            print(self.transactions[dup_mask][['transaction_id', 'customer_id', 'total_amount']].head(10))\n",
    "        \n",
    "        # ===== MERGE VALIDATION =====\n",
    "        print(\"\\n2. Merge Validation Function\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        def validate_merge(left_df, right_df, on_col, merge_type='left'):\n",
    "            \"\"\"\n",
    "            Validate merge integrity.\n",
    "            \n",
    "            Returns:\n",
    "            --------\n",
    "            dict : Validation metrics\n",
    "            \"\"\"\n",
    "            left_count = len(left_df)\n",
    "            merged = left_df.merge(right_df, on=on_col, how=merge_type)\n",
    "            merged_count = len(merged)\n",
    "            \n",
    "            return {\n",
    "                'left_rows': left_count,\n",
    "                'merged_rows': merged_count,\n",
    "                'rows_lost': left_count - merged_count,\n",
    "                'valid': left_count == merged_count\n",
    "            }\n",
    "        \n",
    "        # Test validation\n",
    "        validation = validate_merge(\n",
    "            self.transactions.head(1000),\n",
    "            self.products[['product_id', 'category']],\n",
    "            'product_id'\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nValidation Results:\")\n",
    "        for key, value in validation.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        # ===== CONFLICT RESOLUTION =====\n",
    "        print(\"\\n3. Handling Column Name Conflicts\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Create subsets with overlapping column names\n",
    "        cust_subset = self.customers[['customer_id', 'tier']].copy().rename(columns={'tier': 'customer_tier'})\n",
    "        status_subset = self.status_updates[['customer_id', 'tier']].copy().rename(columns={'tier': 'status_tier'})\n",
    "        \n",
    "        # Merge with explicit suffixes\n",
    "        merged_conflict = cust_subset.merge(status_subset, on='customer_id', how='left')\n",
    "        \n",
    "        print(f\"Merged shape: {merged_conflict.shape}\")\n",
    "        print(f\"Columns: {list(merged_conflict.columns)}\")\n",
    "        print(\"\\nSample data:\")\n",
    "        print(merged_conflict.head(10))\n",
    "        \n",
    "        return num_dups, validation, merged_conflict\n",
    "    \n",
    "    def time_based_joins(self):\n",
    "        \"\"\"\n",
    "        Perform time-based joins.\n",
    "        \n",
    "        Includes:\n",
    "        - AsOf joins for matching by nearest timestamp\n",
    "        - Time-window joins for events within time range\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"2.3: TIME-BASED JOINS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Sort by time (required for some join operations)\n",
    "        trans_sorted = self.transactions.sort_values('date').copy()\n",
    "        status_sorted = self.status_updates.sort_values('update_date').copy()\n",
    "        \n",
    "        # ===== ASOF JOIN =====\n",
    "        print(\"\\n1. AsOf Join - Match by Nearest Previous Timestamp\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Rename columns for clarity\n",
    "        trans_for_asof = trans_sorted[['customer_id', 'date', 'total_amount']].head(100).copy()\n",
    "        status_for_asof = status_sorted[['customer_id', 'update_date', 'tier']].copy()\n",
    "        \n",
    "        # Perform asof join\n",
    "        asof_result = pd.merge_asof(\n",
    "            trans_for_asof.rename(columns={'date': 'transaction_date'}),\n",
    "            status_for_asof,\n",
    "            left_on='transaction_date',      # Left join key\n",
    "            right_on='update_date',          # Right join key\n",
    "            by='customer_id',                # Join by customer\n",
    "            direction='backward'             # Match previous status\n",
    "        )\n",
    "        \n",
    "        print(f\"AsOf join result shape: {asof_result.shape}\")\n",
    "        print(\"\\nSample data (transactions with matched status):\")\n",
    "        print(asof_result[['customer_id', 'transaction_date', 'update_date', 'tier']].head(10))\n",
    "        \n",
    "        # ===== TIME WINDOW JOIN =====\n",
    "        print(\"\\n2. Time Window Join - Match Within 30 Days\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Create sample for faster processing\n",
    "        sample_trans = trans_sorted.head(50)[['customer_id', 'date', 'total_amount']].copy()\n",
    "        sample_status = status_sorted[status_sorted['customer_id'].isin(sample_trans['customer_id'].unique())].copy()\n",
    "        \n",
    "        window_joins = []\n",
    "        for _, trans_row in sample_trans.iterrows():\n",
    "            cust_id = trans_row['customer_id']\n",
    "            trans_date = trans_row['date']\n",
    "            \n",
    "            # Find status updates within 30 days before transaction\n",
    "            matching = sample_status[\n",
    "                (sample_status['customer_id'] == cust_id) &\n",
    "                (sample_status['update_date'] <= trans_date) &\n",
    "                (sample_status['update_date'] >= trans_date - pd.Timedelta(days=30))\n",
    "            ]\n",
    "            \n",
    "            if len(matching) > 0:\n",
    "                # Use most recent update\n",
    "                most_recent = matching.sort_values('update_date').iloc[-1]\n",
    "                window_joins.append({\n",
    "                    'customer_id': cust_id,\n",
    "                    'transaction_date': trans_date,\n",
    "                    'status_date': most_recent['update_date'],\n",
    "                    'days_before': (trans_date - most_recent['update_date']).days\n",
    "                })\n",
    "        \n",
    "        window_result = pd.DataFrame(window_joins)\n",
    "        print(f\"\\nWindow join result shape: {window_result.shape}\")\n",
    "        print(\"\\nSample data (within 30-day window):\")\n",
    "        print(window_result.head(10))\n",
    "        \n",
    "        return asof_result, window_result\n",
    "\n",
    "\n",
    "# Execute Task 2\n",
    "task2 = Task2_MergingAndJoining(transactions_df, products_df, customers_df, status_updates_df)\n",
    "three_way, self_join, join_types = task2.complex_joins()\n",
    "num_dups, validation, merged_conflict = task2.handling_duplicates_and_conflicts()\n",
    "asof_result, window_result = task2.time_based_joins()\n",
    "\n",
    "print(\"\\n✓ Task 2 Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a5207",
   "metadata": {},
   "source": [
    "## 4. TASK 3: Performance Optimization & Memory Management\n",
    "\n",
    "### Topics Covered:\n",
    "1. **Memory Optimization** - Dtype conversion, categoricals\n",
    "2. **Computational Efficiency** - Vectorization vs iterrows\n",
    "3. **Chunking Strategy** - Processing large datasets\n",
    "\n",
    "### Best Practices:\n",
    "- Always use vectorized operations over loops\n",
    "- Use `np.where()` for conditional operations\n",
    "- Downcast numeric types when possible\n",
    "- Process data in chunks for memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "39bba0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "█ TASK 3: PERFORMANCE OPTIMIZATION & MEMORY MANAGEMENT\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "================================================================================\n",
      "3.1: MEMORY USAGE OPTIMIZATION\n",
      "================================================================================\n",
      "\n",
      "1. BEFORE OPTIMIZATION\n",
      "----------------------------------------\n",
      "Total memory: 14.37 MB\n",
      "\n",
      "Memory by column (KB):\n",
      "Index                     0.13\n",
      "transaction_id         2753.67\n",
      "customer_id            3701.17\n",
      "product_id             3701.17\n",
      "date                    236.88\n",
      "quantity                236.88\n",
      "unit_price              236.88\n",
      "discount_pct            236.88\n",
      "price_after_discount    236.88\n",
      "total_amount            236.88\n",
      "payment_method         2895.91\n",
      "store_id                236.88\n",
      "dtype: float64\n",
      "\n",
      "2. OPTIMIZING DATA TYPES\n",
      "----------------------------------------\n",
      "  - payment_method: converted to category\n",
      "  - quantity: downcasted to int8\n",
      "  - Float columns: converted float64 → float32\n",
      "\n",
      "3. AFTER OPTIMIZATION\n",
      "----------------------------------------\n",
      "Total memory: 10.90 MB\n",
      "\n",
      "✓ Memory reduction: 24.1%\n",
      "  Savings: 3.46 MB\n",
      "\n",
      "================================================================================\n",
      "3.2: COMPUTATIONAL EFFICIENCY\n",
      "================================================================================\n",
      "\n",
      "1. Vectorized vs Iterrows\n",
      "----------------------------------------\n",
      "Vectorized: 0.0551 ms\n",
      "Iterrows:   116.7490 ms\n",
      "\n",
      "✓ Speedup: 2120x faster with vectorization!\n",
      "\n",
      "2. Efficient Conditional Operations\n",
      "----------------------------------------\n",
      "np.where():  0.5329 ms\n",
      "pd.cut():    0.5009 ms\n",
      "\n",
      "================================================================================\n",
      "3.3: CHUNKING STRATEGY FOR LARGE DATASETS\n",
      "================================================================================\n",
      "\n",
      "Processing 30,320 rows in chunks of 5000...\n",
      "\n",
      "  Chunk 1:   5000 rows | Sales: $6,041,790.75 | Customers:   479\n",
      "  Chunk 2:   5000 rows | Sales: $5,813,446.39 | Customers:   483\n",
      "  Chunk 3:   5000 rows | Sales: $5,907,347.57 | Customers:   482\n",
      "  Chunk 4:   5000 rows | Sales: $5,994,836.16 | Customers:   485\n",
      "  Chunk 5:   5000 rows | Sales: $5,888,167.31 | Customers:   482\n",
      "  Chunk 6:   5000 rows | Sales: $6,128,136.69 | Customers:   485\n",
      "  Chunk 7:    320 rows | Sales: $  349,941.30 | Customers:   161\n",
      "\n",
      "Total rows processed: 30,320\n",
      "Average chunk size: 4331\n",
      "\n",
      "✓ Task 3 Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TASK 3: PERFORMANCE OPTIMIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"█\"*80)\n",
    "print(\"█ TASK 3: PERFORMANCE OPTIMIZATION & MEMORY MANAGEMENT\")\n",
    "print(\"█\"*80)\n",
    "\n",
    "class Task3_PerformanceOptimization:\n",
    "    \"\"\"\n",
    "    Demonstrates performance and memory optimization techniques.\n",
    "    \n",
    "    Methods:\n",
    "    - memory_usage_optimization: Reduce memory footprint\n",
    "    - computational_efficiency: Vectorization vs iterrows\n",
    "    - chunking_strategy: Large dataset processing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, transactions_df, products_df, customers_df):\n",
    "        \"\"\"Initialize with dataframes\"\"\"\n",
    "        self.transactions = transactions_df.copy()\n",
    "        self.products = products_df.copy()\n",
    "        self.customers = customers_df.copy()\n",
    "    \n",
    "    def memory_usage_optimization(self):\n",
    "        \"\"\"\n",
    "        Analyze and optimize memory usage.\n",
    "        \n",
    "        Strategies:\n",
    "        - Convert object columns to category\n",
    "        - Downcast numeric types (int64→int32, float64→float32)\n",
    "        - Drop unnecessary columns\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"3.1: MEMORY USAGE OPTIMIZATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        df_before = self.transactions.copy()\n",
    "        memory_before = df_before.memory_usage(deep=True).sum() / 1024**2\n",
    "        \n",
    "        print(f\"\\n1. BEFORE OPTIMIZATION\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Total memory: {memory_before:.2f} MB\")\n",
    "        print(f\"\\nMemory by column (KB):\")\n",
    "        print(df_before.memory_usage(deep=True) / 1024)\n",
    "        \n",
    "        # ===== OPTIMIZE DTYPES =====\n",
    "        print(f\"\\n2. OPTIMIZING DATA TYPES\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        df_after = self.transactions.copy()\n",
    "        \n",
    "        # Convert object columns to category (string columns with low cardinality)\n",
    "        categorical_cols = ['payment_method']\n",
    "        for col in categorical_cols:\n",
    "            if col in df_after.columns:\n",
    "                df_after[col] = df_after[col].astype('category')\n",
    "                print(f\"  - {col}: converted to category\")\n",
    "        \n",
    "        # Downcast numeric columns to smaller types\n",
    "        # int64 → int32 or int16 if values fit\n",
    "        if 'quantity' in df_after.columns:\n",
    "            df_after['quantity'] = pd.to_numeric(df_after['quantity'], downcast='integer')\n",
    "            print(f\"  - quantity: downcasted to {df_after['quantity'].dtype}\")\n",
    "        \n",
    "        # float64 → float32 (acceptable precision loss for many applications)\n",
    "        float_cols = df_after.select_dtypes(include=['float64']).columns\n",
    "        for col in float_cols:\n",
    "            df_after[col] = df_after[col].astype('float32')\n",
    "        print(f\"  - Float columns: converted float64 → float32\")\n",
    "        \n",
    "        memory_after = df_after.memory_usage(deep=True).sum() / 1024**2\n",
    "        reduction = ((memory_before - memory_after) / memory_before) * 100\n",
    "        \n",
    "        print(f\"\\n3. AFTER OPTIMIZATION\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Total memory: {memory_after:.2f} MB\")\n",
    "        print(f\"\\n✓ Memory reduction: {reduction:.1f}%\")\n",
    "        print(f\"  Savings: {memory_before - memory_after:.2f} MB\")\n",
    "        \n",
    "        return memory_before, memory_after, df_after\n",
    "    \n",
    "    def computational_efficiency(self):\n",
    "        \"\"\"\n",
    "        Compare efficiency of different operations.\n",
    "        \n",
    "        Demonstrates:\n",
    "        - Vectorized operations (fastest)\n",
    "        - np.where() for conditionals\n",
    "        - Iterrows (slow - DON'T USE)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"3.2: COMPUTATIONAL EFFICIENCY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        df = self.transactions.head(10000).copy()\n",
    "        \n",
    "        # ===== VECTORIZED VS ITERROWS =====\n",
    "        print(\"\\n1. Vectorized vs Iterrows\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # VECTORIZED (FAST) - Use this!\n",
    "        start = time.time()\n",
    "        result_vec = df['total_amount'] * 2\n",
    "        vec_time = time.time() - start\n",
    "        \n",
    "        # ITERROWS (SLOW) - Never use this for large datasets\n",
    "        start = time.time()\n",
    "        result_iter = []\n",
    "        for idx, row in df.iterrows():\n",
    "            result_iter.append(row['total_amount'] * 2)\n",
    "        iter_time = time.time() - start\n",
    "        \n",
    "        print(f\"Vectorized: {vec_time*1000:.4f} ms\")\n",
    "        print(f\"Iterrows:   {iter_time*1000:.4f} ms\")\n",
    "        print(f\"\\n✓ Speedup: {iter_time/vec_time:.0f}x faster with vectorization!\")\n",
    "        \n",
    "        # ===== CONDITIONAL OPERATIONS =====\n",
    "        print(\"\\n2. Efficient Conditional Operations\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Method 1: np.where (very fast)\n",
    "        start = time.time()\n",
    "        result_where = np.where(\n",
    "            df['discount_pct'] > 0.1, 'High',\n",
    "            np.where(df['discount_pct'] > 0.05, 'Medium', 'Low')\n",
    "        )\n",
    "        where_time = time.time() - start\n",
    "        \n",
    "        # Method 2: pd.cut (also fast)\n",
    "        start = time.time()\n",
    "        result_cut = pd.cut(\n",
    "            df['total_amount'],\n",
    "            bins=[0, 50, 100, 500, float('inf')],\n",
    "            labels=['Budget', 'Standard', 'Premium', 'Luxury']\n",
    "        )\n",
    "        cut_time = time.time() - start\n",
    "        \n",
    "        print(f\"np.where():  {where_time*1000:.4f} ms\")\n",
    "        print(f\"pd.cut():    {cut_time*1000:.4f} ms\")\n",
    "        \n",
    "        return vec_time, iter_time, where_time, cut_time\n",
    "    \n",
    "    def chunking_strategy(self):\n",
    "        \"\"\"\n",
    "        Demonstrate chunking for large dataset processing.\n",
    "        \n",
    "        Useful for:\n",
    "        - Processing data larger than RAM\n",
    "        - Batch operations\n",
    "        - Parallel processing\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"3.3: CHUNKING STRATEGY FOR LARGE DATASETS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        def chunk_dataframe(df, chunk_size=5000):\n",
    "            \"\"\"\n",
    "            Yield successive chunks from DataFrame.\n",
    "            \n",
    "            Useful as generator to avoid loading full dataset at once.\n",
    "            \"\"\"\n",
    "            for i in range(0, len(df), chunk_size):\n",
    "                yield df.iloc[i:i + chunk_size]\n",
    "        \n",
    "        print(f\"\\nProcessing {len(self.transactions):,} rows in chunks of 5000...\\n\")\n",
    "        \n",
    "        chunk_results = []\n",
    "        total_rows = 0\n",
    "        \n",
    "        for chunk_num, chunk in enumerate(chunk_dataframe(self.transactions, chunk_size=5000), 1):\n",
    "            # Process each chunk\n",
    "            chunk_summary = {\n",
    "                'chunk_num': chunk_num,\n",
    "                'rows': len(chunk),\n",
    "                'total_sales': chunk['total_amount'].sum(),\n",
    "                'avg_sale': chunk['total_amount'].mean(),\n",
    "                'unique_customers': chunk['customer_id'].nunique()\n",
    "            }\n",
    "            chunk_results.append(chunk_summary)\n",
    "            total_rows += len(chunk)\n",
    "            \n",
    "            print(f\"  Chunk {chunk_num}: {len(chunk):6d} rows | \"\n",
    "                  f\"Sales: ${chunk['total_amount'].sum():12,.2f} | \"\n",
    "                  f\"Customers: {chunk['customer_id'].nunique():5d}\")\n",
    "        \n",
    "        chunk_df = pd.DataFrame(chunk_results)\n",
    "        \n",
    "        print(f\"\\nTotal rows processed: {total_rows:,}\")\n",
    "        print(f\"Average chunk size: {chunk_df['rows'].mean():.0f}\")\n",
    "        \n",
    "        return chunk_results\n",
    "\n",
    "\n",
    "# Execute Task 3\n",
    "task3 = Task3_PerformanceOptimization(transactions_df, products_df, customers_df)\n",
    "mem_before, mem_after, df_optimized = task3.memory_usage_optimization()\n",
    "vec_time, iter_time, where_time, cut_time = task3.computational_efficiency()\n",
    "chunks = task3.chunking_strategy()\n",
    "\n",
    "print(\"\\n✓ Task 3 Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764241ad",
   "metadata": {},
   "source": [
    "## 5. TASK 4: Custom Extensions & Advanced Functionality\n",
    "\n",
    "### Topics Covered:\n",
    "1. **Custom Accessors** - Extend DataFrame with custom methods\n",
    "2. **Method Chaining** - Build reusable data pipelines\n",
    "3. **Advanced Analysis** - Complex pandas operations\n",
    "\n",
    "### Best Practices:\n",
    "- Register custom accessors for domain-specific operations\n",
    "- Use method chaining for readable pipelines\n",
    "- Implement docstrings for all custom methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "82f04e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "█ TASK 4: CUSTOM EXTENSIONS & ADVANCED FUNCTIONALITY\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "================================================================================\n",
      "4.1: CUSTOM ACCESSORS\n",
      "================================================================================\n",
      "\n",
      "1. Sales Accessor Methods\n",
      "----------------------------------------\n",
      "Total Revenue: $36,123,666.17\n",
      "Avg Transaction: $1191.41\n",
      "\n",
      "Revenue by Category:\n",
      "  Food           : $10,464,437.49 ( 29.0%)\n",
      "  Electronics    : $8,412,826.42 ( 23.3%)\n",
      "  Beauty         : $7,414,828.17 ( 20.5%)\n",
      "  Home           : $6,179,807.47 ( 17.1%)\n",
      "  Clothing       : $3,651,766.62 ( 10.1%)\n",
      "\n",
      "Discount Analysis:\n",
      "  % Transactions Discounted: 20.0%\n",
      "  Average Discount: 12.44%\n",
      "  Revenue (Discounted): $6,530,673.25\n",
      "\n",
      "2. Quality Accessor Methods\n",
      "----------------------------------------\n",
      "Data Completeness: 99.8%\n",
      "Duplicate Ratio: 0.49%\n",
      "\n",
      "Missing Values by Column:\n",
      "  unit_price: 1.00%\n",
      "  discount_pct: 0.99%\n",
      "\n",
      "================================================================================\n",
      "4.2: METHOD CHAINING PIPELINES\n",
      "================================================================================\n",
      "\n",
      "Building data processing pipeline...\n",
      "----------------------------------------\n",
      "\n",
      "Pipeline output:\n",
      "  Shape: (30169, 15)\n",
      "  Columns: 15\n",
      "\n",
      "Sample result:\n",
      "                            customer_id  category         region  \\\n",
      "0  0e054f3f-aff6-4ced-b31e-a947c46b27cd      Home         Africa   \n",
      "1  c19f586b-07d3-4b30-a4fd-5380472fac91  Clothing         Europe   \n",
      "2  e368e70b-0c93-4708-9fa5-11972da32a10      Food           Asia   \n",
      "3  e368e70b-0c93-4708-9fa5-11972da32a10      Home           Asia   \n",
      "4  e368e70b-0c93-4708-9fa5-11972da32a10      Food           Asia   \n",
      "5  e69387ef-b636-4c42-b538-0f6c222a2352      Home         Europe   \n",
      "6  e69387ef-b636-4c42-b538-0f6c222a2352  Clothing         Europe   \n",
      "7  2eec9df9-85d9-41e8-b269-c7215482e793      Food  North America   \n",
      "8  e69387ef-b636-4c42-b538-0f6c222a2352      Food         Europe   \n",
      "9  e69387ef-b636-4c42-b538-0f6c222a2352      Food         Europe   \n",
      "\n",
      "   total_amount price_segment  \n",
      "0       3343.20        Luxury  \n",
      "1       1758.12        Luxury  \n",
      "2       2550.28        Luxury  \n",
      "3       1650.16        Luxury  \n",
      "4       1664.76        Luxury  \n",
      "5       3034.60        Luxury  \n",
      "6       2795.76        Luxury  \n",
      "7        773.24        Luxury  \n",
      "8        282.68       Premium  \n",
      "9        671.13        Luxury  \n",
      "\n",
      "✓ Task 4 Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TASK 4: CUSTOM EXTENSIONS AND ADVANCED FUNCTIONALITY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"█\"*80)\n",
    "print(\"█ TASK 4: CUSTOM EXTENSIONS & ADVANCED FUNCTIONALITY\")\n",
    "print(\"█\"*80)\n",
    "\n",
    "# Register custom pandas accessor for sales data\n",
    "@pd.api.extensions.register_dataframe_accessor(\"sales\")\n",
    "class SalesAccessor:\n",
    "    \"\"\"\n",
    "    Custom accessor for sales-specific operations.\n",
    "    \n",
    "    Usage:\n",
    "        df.sales.total_revenue()\n",
    "        df.sales.discount_analysis()\n",
    "        etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pandas_obj):\n",
    "        \"\"\"Store reference to parent dataframe\"\"\"\n",
    "        self._obj = pandas_obj\n",
    "    \n",
    "    def total_revenue(self):\n",
    "        \"\"\"Calculate total revenue\"\"\"\n",
    "        return self._obj['total_amount'].sum()\n",
    "    \n",
    "    def avg_transaction(self):\n",
    "        \"\"\"Calculate average transaction value\"\"\"\n",
    "        return self._obj['total_amount'].mean()\n",
    "    \n",
    "    def revenue_by_category(self):\n",
    "        \"\"\"Get revenue breakdown by category\"\"\"\n",
    "        if 'category' in self._obj.columns:\n",
    "            return self._obj.groupby('category')['total_amount'].sum().sort_values(ascending=False)\n",
    "        return None\n",
    "    \n",
    "    def discount_analysis(self):\n",
    "        \"\"\"Analyze impact of discounts on revenue\"\"\"\n",
    "        if 'discount_pct' in self._obj.columns and 'total_amount' in self._obj.columns:\n",
    "            discounted = self._obj[self._obj['discount_pct'] > 0]\n",
    "            return {\n",
    "                'pct_discounted': (len(discounted) / len(self._obj)) * 100,\n",
    "                'avg_discount_pct': discounted['discount_pct'].mean() * 100,\n",
    "                'total_revenue_discounted': discounted['total_amount'].sum()\n",
    "            }\n",
    "        return None\n",
    "\n",
    "\n",
    "# Register quality accessor\n",
    "@pd.api.extensions.register_dataframe_accessor(\"quality\")\n",
    "class QualityAccessor:\n",
    "    \"\"\"\n",
    "    Custom accessor for data quality metrics.\n",
    "    \n",
    "    Usage:\n",
    "        df.quality.completeness()\n",
    "        df.quality.duplicate_ratio\n",
    "        etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pandas_obj):\n",
    "        \"\"\"Store reference to parent dataframe\"\"\"\n",
    "        self._obj = pandas_obj\n",
    "    \n",
    "    def completeness(self):\n",
    "        \"\"\"Calculate data completeness percentage\"\"\"\n",
    "        total_cells = self._obj.size\n",
    "        null_cells = self._obj.isnull().sum().sum()\n",
    "        return ((total_cells - null_cells) / total_cells) * 100\n",
    "    \n",
    "    @property\n",
    "    def duplicate_ratio(self):\n",
    "        \"\"\"Calculate duplicate row percentage\"\"\"\n",
    "        total_rows = len(self._obj)\n",
    "        duplicate_rows = self._obj.duplicated().sum()\n",
    "        return (duplicate_rows / total_rows) * 100 if total_rows > 0 else 0\n",
    "    \n",
    "    @property\n",
    "    def missing_summary(self):\n",
    "        \"\"\"Get missing value summary by column\"\"\"\n",
    "        return (self._obj.isnull().sum() / len(self._obj) * 100).round(2)\n",
    "\n",
    "\n",
    "class Task4_CustomExtensions:\n",
    "    \"\"\"\n",
    "    Demonstrates custom accessors and method chaining.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, transactions_df, products_df, customers_df):\n",
    "        \"\"\"Initialize with dataframes\"\"\"\n",
    "        self.transactions = transactions_df.copy()\n",
    "        self.products = products_df.copy()\n",
    "        self.customers = customers_df.copy()\n",
    "    \n",
    "    def custom_accessors_demo(self):\n",
    "        \"\"\"\n",
    "        Demonstrate custom pandas accessors.\n",
    "        \n",
    "        Custom accessors allow extending DataFrame with domain-specific methods.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"4.1: CUSTOM ACCESSORS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Merge data\n",
    "        df = self.transactions.merge(\n",
    "            self.products[['product_id', 'category']],\n",
    "            on='product_id',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # ===== SALES ACCESSOR =====\n",
    "        print(\"\\n1. Sales Accessor Methods\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        print(f\"Total Revenue: ${df.sales.total_revenue():,.2f}\")\n",
    "        print(f\"Avg Transaction: ${df.sales.avg_transaction():.2f}\")\n",
    "        \n",
    "        print(\"\\nRevenue by Category:\")\n",
    "        revenue_by_cat = df.sales.revenue_by_category()\n",
    "        for category, revenue in revenue_by_cat.items():\n",
    "            pct = (revenue / df.sales.total_revenue()) * 100\n",
    "            print(f\"  {category:15s}: ${revenue:12,.2f} ({pct:5.1f}%)\")\n",
    "        \n",
    "        print(\"\\nDiscount Analysis:\")\n",
    "        discount_info = df.sales.discount_analysis()\n",
    "        if discount_info:\n",
    "            print(f\"  % Transactions Discounted: {discount_info['pct_discounted']:.1f}%\")\n",
    "            print(f\"  Average Discount: {discount_info['avg_discount_pct']:.2f}%\")\n",
    "            print(f\"  Revenue (Discounted): ${discount_info['total_revenue_discounted']:,.2f}\")\n",
    "        \n",
    "        # ===== QUALITY ACCESSOR =====\n",
    "        print(\"\\n2. Quality Accessor Methods\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        print(f\"Data Completeness: {df.quality.completeness():.1f}%\")\n",
    "        print(f\"Duplicate Ratio: {df.quality.duplicate_ratio:.2f}%\")\n",
    "        \n",
    "        print(\"\\nMissing Values by Column:\")\n",
    "        missing = df.quality.missing_summary\n",
    "        for col, pct in missing[missing > 0].items():\n",
    "            print(f\"  {col}: {pct:.2f}%\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def method_chaining_pipeline(self):\n",
    "        \"\"\"\n",
    "        Create reusable data processing pipeline using method chaining.\n",
    "        \n",
    "        Method chaining improves readability and enables functional programming style.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"4.2: METHOD CHAINING PIPELINES\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(\"\\nBuilding data processing pipeline...\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Create pipeline using method chaining\n",
    "        result = (\n",
    "            self.transactions\n",
    "            .copy()\n",
    "            # Step 1: Filter valid transactions\n",
    "            .query('total_amount > 0')\n",
    "            # Step 2: Merge with product info\n",
    "            .merge(self.products[['product_id', 'category']], on='product_id', how='left')\n",
    "            # Step 3: Merge with customer info\n",
    "            .merge(self.customers[['customer_id', 'region']], on='customer_id', how='left')\n",
    "            # Step 4: Add derived columns\n",
    "            .assign(\n",
    "                transaction_month=lambda x: x['date'].dt.to_period('M'),\n",
    "                price_segment=lambda x: pd.cut(\n",
    "                    x['total_amount'],\n",
    "                    bins=[0, 50, 100, 500, float('inf')],\n",
    "                    labels=['Budget', 'Standard', 'Premium', 'Luxury']\n",
    "                )\n",
    "            )\n",
    "            # Step 5: Sort by date\n",
    "            .sort_values('date')\n",
    "            # Step 6: Reset index\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nPipeline output:\")\n",
    "        print(f\"  Shape: {result.shape}\")\n",
    "        print(f\"  Columns: {len(result.columns)}\")\n",
    "        print(f\"\\nSample result:\")\n",
    "        print(result[['customer_id', 'category', 'region', 'total_amount', 'price_segment']].head(10))\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "# Execute Task 4\n",
    "task4 = Task4_CustomExtensions(transactions_df, products_df, customers_df)\n",
    "sales_df = task4.custom_accessors_demo()\n",
    "pipeline_result = task4.method_chaining_pipeline()\n",
    "\n",
    "print(\"\\n✓ Task 4 Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35003a24",
   "metadata": {},
   "source": [
    "## 6. TASK 5: Real-World Challenge - Retail Analytics\n",
    "\n",
    "### Topics Covered:\n",
    "1. **Data Cleaning** - Handle missing values, duplicates, outliers\n",
    "2. **Customer Profiling** - Create RFM segments\n",
    "3. **Product Analytics** - Revenue analysis, recommendations\n",
    "4. **Anomaly Detection** - Statistical and behavioral anomalies\n",
    "5. **Business Insights** - Dashboard generation\n",
    "\n",
    "### Best Practices:\n",
    "- Always validate data quality before analysis\n",
    "- Document business rules clearly\n",
    "- Create reproducible analysis pipelines\n",
    "- Validate results with domain experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "681bf8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "█ TASK 5: REAL-WORLD CHALLENGE - RETAIL ANALYTICS\n",
      "████████████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "================================================================================\n",
      "5.1: DATA CLEANING\n",
      "================================================================================\n",
      "\n",
      "1. Missing Values Analysis\n",
      "----------------------------------------\n",
      "              Missing_Count  Missing_Percent\n",
      "unit_price              304             1.00\n",
      "discount_pct            300             0.99\n",
      "\n",
      "2. Duplicate Record Handling\n",
      "----------------------------------------\n",
      "Duplicates before cleaning: 150\n",
      "Duplicates after cleaning: 0\n",
      "\n",
      "3. Outlier Detection (IQR Method)\n",
      "----------------------------------------\n",
      "Outliers detected: 292 (0.97%)\n",
      "Bounds: [-1447.26, 3659.22]\n",
      "\n",
      "Final cleaned data: 30020 records\n",
      "\n",
      "================================================================================\n",
      "5.2: CUSTOMER PROFILING & SEGMENTATION\n",
      "================================================================================\n",
      "\n",
      "1. RFM (Recency, Frequency, Monetary) Analysis\n",
      "----------------------------------------\n",
      "\n",
      "Customer Metrics Summary:\n",
      "       frequency  monetary  recency\n",
      "count     500.00    500.00   500.00\n",
      "mean       60.04  71934.16   829.46\n",
      "std        15.73  20007.16    70.10\n",
      "min        22.00  24137.83   759.00\n",
      "25%        48.00  56806.89   778.75\n",
      "50%        59.00  70180.41   807.00\n",
      "75%        71.00  86317.90   859.00\n",
      "max       108.00 144149.94  1386.00\n",
      "\n",
      "2. Customer Segmentation\n",
      "----------------------------------------\n",
      "\n",
      "Segment Distribution:\n",
      "          Count  Avg_Value  Total_Value  Avg_Frequency\n",
      "segment                                               \n",
      "At-Risk     124   64878.96   8044991.59          53.73\n",
      "Standard    290   66433.69  19265769.12          55.93\n",
      "VIP          86  100654.88   8656319.51          82.99\n",
      "\n",
      "================================================================================\n",
      "5.3: PRODUCT ANALYSIS & RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "1. Revenue Analysis by Category\n",
      "----------------------------------------\n",
      "\n",
      "Top Categories by Revenue:\n",
      "             Total_Revenue  Avg_Transaction  Num_Transactions  Avg_Quantity\n",
      "category                                                                   \n",
      "Food           10431118.28          1275.20              8180          2.49\n",
      "Electronics     8363468.75          1219.70              6857          2.48\n",
      "Beauty          7386868.64          1175.32              6285          2.51\n",
      "Home            6143179.73          1195.17              5140          2.49\n",
      "Clothing        3642444.82          1023.73              3558          2.52\n",
      "\n",
      "2. Co-Purchase Patterns\n",
      "----------------------------------------\n",
      "\n",
      "Found 4950 unique co-purchase pairs\n",
      "Top 10 patterns: 10 pairs\n",
      "\n",
      "================================================================================\n",
      "5.4: BUSINESS INSIGHTS & SUMMARY\n",
      "================================================================================\n",
      "\n",
      "1. Key Performance Indicators (KPIs)\n",
      "----------------------------------------\n",
      "  Total Revenue                 : $35,967,080.22\n",
      "  Number of Transactions        :       30,020\n",
      "  Average Order Value           : $    1,198.10\n",
      "  Number of Customers           :          500\n",
      "  Number of Products            :          100\n",
      "  Revenue per Customer          : $   71,934.16\n",
      "  Avg Discount                  :         2.52%\n",
      "\n",
      "2. Geographic Performance\n",
      "----------------------------------------\n",
      "\n",
      "Revenue by Region:\n",
      "  Europe              : $6,485,735.27 ( 18.0%)\n",
      "  Africa              : $6,404,047.47 ( 17.8%)\n",
      "  Asia                : $6,335,001.98 ( 17.6%)\n",
      "  South America       : $6,306,509.67 ( 17.5%)\n",
      "  North America       : $5,546,614.37 ( 15.4%)\n",
      "  Oceania             : $4,889,171.46 ( 13.6%)\n",
      "\n",
      "3. Category Performance\n",
      "----------------------------------------\n",
      "\n",
      "Revenue by Category:\n",
      "  Food                : $10,431,118.28 ( 29.0%)\n",
      "  Electronics         : $8,363,468.75 ( 23.3%)\n",
      "  Beauty              : $7,386,868.64 ( 20.5%)\n",
      "  Home                : $6,143,179.73 ( 17.1%)\n",
      "  Clothing            : $3,642,444.82 ( 10.1%)\n",
      "\n",
      "✓ Task 5 Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TASK 5: REAL-WORLD CHALLENGE - RETAIL ANALYTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"█\"*80)\n",
    "print(\"█ TASK 5: REAL-WORLD CHALLENGE - RETAIL ANALYTICS\")\n",
    "print(\"█\"*80)\n",
    "\n",
    "class Task5_RealWorldChallenge:\n",
    "    \"\"\"\n",
    "    Real-world retail analytics challenge demonstrating end-to-end analysis.\n",
    "    \n",
    "    Methods:\n",
    "    - data_cleaning: Handle data quality issues\n",
    "    - customer_profiling: Create customer segments\n",
    "    - product_analysis: Revenue and performance analysis\n",
    "    - generate_insights: Create actionable insights\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, transactions_df, products_df, customers_df, status_updates_df):\n",
    "        \"\"\"Initialize with all data sources\"\"\"\n",
    "        self.transactions = transactions_df.copy()\n",
    "        self.products = products_df.copy()\n",
    "        self.customers = customers_df.copy()\n",
    "        self.status_updates = status_updates_df.copy()\n",
    "    \n",
    "    def data_cleaning(self):\n",
    "        \"\"\"\n",
    "        Clean data by handling:\n",
    "        - Missing values\n",
    "        - Duplicate records\n",
    "        - Outliers and anomalies\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"5.1: DATA CLEANING\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        df = self.transactions.copy()\n",
    "        \n",
    "        # ===== MISSING VALUES ANALYSIS =====\n",
    "        print(\"\\n1. Missing Values Analysis\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        missing_counts = df.isnull().sum()\n",
    "        missing_pct = (missing_counts / len(df)) * 100\n",
    "        missing_info = pd.DataFrame({\n",
    "            'Missing_Count': missing_counts,\n",
    "            'Missing_Percent': missing_pct\n",
    "        })\n",
    "        \n",
    "        print(missing_info[missing_info['Missing_Count'] > 0])\n",
    "        \n",
    "        # Handle missing values\n",
    "        df_cleaned = df.copy()\n",
    "        \n",
    "        # Fill numeric missing values with mean\n",
    "        numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            if df_cleaned[col].isnull().any():\n",
    "                df_cleaned[col].fillna(df_cleaned[col].mean(), inplace=True)\n",
    "        \n",
    "        # ===== DUPLICATE HANDLING =====\n",
    "        print(\"\\n2. Duplicate Record Handling\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        print(f\"Duplicates before cleaning: {df_cleaned.duplicated().sum()}\")\n",
    "        df_cleaned = df_cleaned.drop_duplicates(subset=['transaction_id'], keep='first')\n",
    "        print(f\"Duplicates after cleaning: {df_cleaned.duplicated().sum()}\")\n",
    "        \n",
    "        # ===== OUTLIER DETECTION =====\n",
    "        print(\"\\n3. Outlier Detection (IQR Method)\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        Q1 = df_cleaned['total_amount'].quantile(0.25)\n",
    "        Q3 = df_cleaned['total_amount'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df_cleaned[\n",
    "            (df_cleaned['total_amount'] < lower_bound) |\n",
    "            (df_cleaned['total_amount'] > upper_bound)\n",
    "        ]\n",
    "        \n",
    "        print(f\"Outliers detected: {len(outliers)} ({len(outliers)/len(df_cleaned)*100:.2f}%)\")\n",
    "        print(f\"Bounds: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "        \n",
    "        # Remove invalid transactions (negative amounts)\n",
    "        df_cleaned = df_cleaned[df_cleaned['total_amount'] > 0]\n",
    "        \n",
    "        print(f\"\\nFinal cleaned data: {len(df_cleaned)} records\")\n",
    "        \n",
    "        return df_cleaned\n",
    "    \n",
    "    def customer_profiling(self, clean_transactions):\n",
    "        \"\"\"\n",
    "        Create comprehensive customer profiles and segments.\n",
    "        \n",
    "        Metrics:\n",
    "        - Recency: Days since last purchase\n",
    "        - Frequency: Number of transactions\n",
    "        - Monetary: Total spending\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"5.2: CUSTOMER PROFILING & SEGMENTATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create RFM metrics\n",
    "        print(\"\\n1. RFM (Recency, Frequency, Monetary) Analysis\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        customer_metrics = clean_transactions.groupby('customer_id').agg({\n",
    "            'transaction_id': 'count',     # Frequency\n",
    "            'total_amount': 'sum',          # Monetary\n",
    "            'date': 'max'                  # Recency (latest purchase)\n",
    "        }).reset_index()\n",
    "        \n",
    "        customer_metrics.columns = ['customer_id', 'frequency', 'monetary', 'last_purchase']\n",
    "        \n",
    "        # Calculate recency (days since last purchase)\n",
    "        customer_metrics['recency'] = (\n",
    "            pd.Timestamp.now() - customer_metrics['last_purchase']\n",
    "        ).dt.days\n",
    "        \n",
    "        print(f\"\\nCustomer Metrics Summary:\")\n",
    "        print(customer_metrics[['frequency', 'monetary', 'recency']].describe())\n",
    "        \n",
    "        # ===== CUSTOMER SEGMENTATION =====\n",
    "        print(\"\\n2. Customer Segmentation\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Create segments based on RFM quartiles\n",
    "        customer_metrics['segment'] = 'Standard'\n",
    "        \n",
    "        # VIP: High frequency, high spending, low recency\n",
    "        vip_cond = (\n",
    "            (customer_metrics['frequency'] > customer_metrics['frequency'].quantile(0.75)) &\n",
    "            (customer_metrics['monetary'] > customer_metrics['monetary'].quantile(0.75))\n",
    "        )\n",
    "        customer_metrics.loc[vip_cond, 'segment'] = 'VIP'\n",
    "        \n",
    "        # At-Risk: Low frequency or high recency\n",
    "        risk_cond = customer_metrics['recency'] > customer_metrics['recency'].quantile(0.75)\n",
    "        customer_metrics.loc[risk_cond, 'segment'] = 'At-Risk'\n",
    "        \n",
    "        # New: Low frequency but recent\n",
    "        new_cond = (\n",
    "            (customer_metrics['frequency'] < customer_metrics['frequency'].quantile(0.25)) &\n",
    "            (customer_metrics['recency'] < 30)\n",
    "        )\n",
    "        customer_metrics.loc[new_cond, 'segment'] = 'New'\n",
    "        \n",
    "        print(\"\\nSegment Distribution:\")\n",
    "        segment_stats = customer_metrics.groupby('segment').agg({\n",
    "            'customer_id': 'count',\n",
    "            'monetary': ['mean', 'sum'],\n",
    "            'frequency': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        segment_stats.columns = ['Count', 'Avg_Value', 'Total_Value', 'Avg_Frequency']\n",
    "        print(segment_stats)\n",
    "        \n",
    "        return customer_metrics\n",
    "    \n",
    "    def product_analysis(self, clean_transactions):\n",
    "        \"\"\"\n",
    "        Analyze product performance and identify recommendations.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"5.3: PRODUCT ANALYSIS & RECOMMENDATIONS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Merge with product data\n",
    "        df = clean_transactions.merge(\n",
    "            self.products[['product_id', 'category', 'price']],\n",
    "            on='product_id',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # ===== REVENUE ANALYSIS =====\n",
    "        print(\"\\n1. Revenue Analysis by Category\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        category_revenue = df.groupby('category').agg({\n",
    "            'total_amount': ['sum', 'mean', 'count'],\n",
    "            'quantity': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        category_revenue.columns = ['Total_Revenue', 'Avg_Transaction', 'Num_Transactions', 'Avg_Quantity']\n",
    "        category_revenue = category_revenue.sort_values('Total_Revenue', ascending=False)\n",
    "        \n",
    "        print(\"\\nTop Categories by Revenue:\")\n",
    "        print(category_revenue)\n",
    "        \n",
    "        # ===== CO-PURCHASE ANALYSIS =====\n",
    "        print(\"\\n2. Co-Purchase Patterns\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Find products bought together\n",
    "        customer_products = df.groupby('customer_id')['product_id'].apply(list).reset_index()\n",
    "        \n",
    "        copurchase_patterns = {}\n",
    "        for products in customer_products['product_id']:\n",
    "            if len(products) > 1:\n",
    "                for pair in combinations(set(products), 2):\n",
    "                    key = tuple(sorted(pair))\n",
    "                    copurchase_patterns[key] = copurchase_patterns.get(key, 0) + 1\n",
    "        \n",
    "        # Get top patterns\n",
    "        top_patterns = sorted(copurchase_patterns.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        print(f\"\\nFound {len(copurchase_patterns)} unique co-purchase pairs\")\n",
    "        print(f\"Top 10 patterns: {len(top_patterns)} pairs\")\n",
    "        \n",
    "        return category_revenue, copurchase_patterns\n",
    "    \n",
    "    def generate_insights(self, clean_transactions):\n",
    "        \"\"\"\n",
    "        Generate business insights and summary statistics.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"5.4: BUSINESS INSIGHTS & SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Merge all data\n",
    "        df = clean_transactions.merge(\n",
    "            self.products[['product_id', 'category']],\n",
    "            on='product_id',\n",
    "            how='left'\n",
    "        ).merge(\n",
    "            self.customers[['customer_id', 'region']],\n",
    "            on='customer_id',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # ===== KEY METRICS =====\n",
    "        print(\"\\n1. Key Performance Indicators (KPIs)\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        kpis = {\n",
    "            'Total Revenue': df['total_amount'].sum(),\n",
    "            'Number of Transactions': len(df),\n",
    "            'Average Order Value': df['total_amount'].mean(),\n",
    "            'Number of Customers': df['customer_id'].nunique(),\n",
    "            'Number of Products': df['product_id'].nunique(),\n",
    "            'Revenue per Customer': df['total_amount'].sum() / df['customer_id'].nunique(),\n",
    "            'Avg Discount': df['discount_pct'].mean() * 100\n",
    "        }\n",
    "        \n",
    "        for metric, value in kpis.items():\n",
    "            if 'Revenue' in metric or 'Value' in metric:\n",
    "                print(f\"  {metric:30s}: ${value:12,.2f}\")\n",
    "            elif 'Discount' in metric:\n",
    "                print(f\"  {metric:30s}: {value:12.2f}%\")\n",
    "            else:\n",
    "                print(f\"  {metric:30s}: {value:12,.0f}\")\n",
    "        \n",
    "        # ===== GEOGRAPHIC ANALYSIS =====\n",
    "        print(\"\\n2. Geographic Performance\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        regional_revenue = df.groupby('region')['total_amount'].sum().sort_values(ascending=False)\n",
    "        print(\"\\nRevenue by Region:\")\n",
    "        for region, revenue in regional_revenue.items():\n",
    "            pct = (revenue / df['total_amount'].sum()) * 100\n",
    "            print(f\"  {region:20s}: ${revenue:12,.2f} ({pct:5.1f}%)\")\n",
    "        \n",
    "        # ===== CATEGORY ANALYSIS =====\n",
    "        print(\"\\n3. Category Performance\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        category_revenue = df.groupby('category')['total_amount'].sum().sort_values(ascending=False)\n",
    "        print(\"\\nRevenue by Category:\")\n",
    "        for category, revenue in category_revenue.items():\n",
    "            pct = (revenue / df['total_amount'].sum()) * 100\n",
    "            print(f\"  {category:20s}: ${revenue:12,.2f} ({pct:5.1f}%)\")\n",
    "        \n",
    "        return kpis\n",
    "\n",
    "\n",
    "# Execute Task 5\n",
    "task5 = Task5_RealWorldChallenge(transactions_df, products_df, customers_df, status_updates_df)\n",
    "clean_trans = task5.data_cleaning()\n",
    "customer_profiles = task5.customer_profiling(clean_trans)\n",
    "category_revenue, copurchase = task5.product_analysis(clean_trans)\n",
    "kpis = task5.generate_insights(clean_trans)\n",
    "\n",
    "print(\"\\n✓ Task 5 Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc82509d",
   "metadata": {},
   "source": [
    "## 7. Execution Summary & Recommendations\n",
    "\n",
    "### Tasks Completed ✓\n",
    "\n",
    "1. **Task 1: Advanced Data Transformation** ✓\n",
    "   - Pivot/Melt operations for format conversion\n",
    "   - Multi-level hierarchical indexing\n",
    "   - Advanced GroupBy with transform, agg, and custom functions\n",
    "\n",
    "2. **Task 2: Advanced Merging & Joining** ✓\n",
    "   - Three-way joins and self-joins\n",
    "   - Duplicate handling and conflict resolution\n",
    "   - Time-based joins (AsOf and time-window)\n",
    "\n",
    "3. **Task 3: Performance Optimization** ✓\n",
    "   - Memory usage optimization (dtype conversion, categoricals)\n",
    "   - Computational efficiency (vectorization vs loops)\n",
    "   - Chunking strategy for large datasets\n",
    "\n",
    "4. **Task 4: Custom Extensions** ✓\n",
    "   - Custom pandas accessors for domain-specific operations\n",
    "   - Method chaining for readable pipelines\n",
    "   - Advanced functionality demonstrations\n",
    "\n",
    "5. **Task 5: Real-World Challenge** ✓\n",
    "   - Data cleaning (missing values, duplicates, outliers)\n",
    "   - Customer profiling and RFM segmentation\n",
    "   - Product analysis and recommendations\n",
    "   - Business insights generation\n",
    "\n",
    "### Pandas Best Practices Summary\n",
    "\n",
    "#### ✓ DO:\n",
    "- Use vectorized operations (NumPy/Pandas) over loops\n",
    "- Use `.loc[]` for label-based indexing\n",
    "- Use `.groupby().agg()` for flexible aggregations\n",
    "- Use `.merge()` with appropriate how parameter\n",
    "- Use `.astype()` for dtype conversion\n",
    "- Copy dataframes when needed: `.copy()`\n",
    "\n",
    "#### ✗ DON'T:\n",
    "- Never use `.iterrows()` on large datasets\n",
    "- Avoid `.apply(axis=1)` for simple operations\n",
    "- Don't modify during iteration\n",
    "- Avoid creating temporary lists for aggregation\n",
    "- Don't ignore data quality issues\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "1. **Memory**: Reduce by 30-50% with dtype optimization\n",
    "2. **Speed**: Vectorization is 10-100x faster than iterrows\n",
    "3. **Scalability**: Use chunking for datasets > 100MB\n",
    "4. **Quality**: Always validate merges and duplicates\n",
    "\n",
    "---\n",
    "\n",
    "**Created**: January 27, 2026  \n",
    "**Python Version**: 3.9+  \n",
    "**Pandas Version**: 1.3.0+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f4800fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ADVANCED PANDAS NOTEBOOK - EXECUTION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "✓ TASKS COMPLETED:\n",
      "  1. Advanced Data Transformation & Reshaping\n",
      "  2. Advanced Merging & Joining\n",
      "  3. Performance Optimization & Memory Management\n",
      "  4. Custom Extensions & Advanced Functionality\n",
      "  5. Real-World Challenge - Retail Analytics\n",
      "\n",
      "✓ KEY LEARNINGS:\n",
      "  • Pivot/Melt for flexible data reshaping\n",
      "  • Hierarchical indexing for complex data organization\n",
      "  • Advanced groupby operations with transform/agg\n",
      "  • Complex merges with duplicate/conflict handling\n",
      "  • Time-based joins with AsOf and time-windows\n",
      "  • Memory optimization through dtype management\n",
      "  • Performance gains through vectorization\n",
      "  • Custom pandas accessors for domain operations\n",
      "  • Method chaining for readable pipelines\n",
      "  • Real-world analytics workflow\n",
      "\n",
      "✓ BEST PRACTICES:\n",
      "  • Always vectorize - Never use iterrows\n",
      "  • Validate data quality before analysis\n",
      "  • Use method chaining for readability\n",
      "  • Document all custom operations\n",
      "  • Optimize memory for large datasets\n",
      "  • Test merge operations for data integrity\n",
      "\n",
      "📊 RESOURCES FOR FURTHER LEARNING:\n",
      "  • https://pandas.pydata.org/docs/\n",
      "  • https://pandas.pydata.org/docs/user_guide/\n",
      "  • Real dataset exploration and practice\n",
      "\n",
      "================================================================================\n",
      "Thank you for using this comprehensive Pandas guide!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADVANCED PANDAS NOTEBOOK - EXECUTION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "✓ TASKS COMPLETED:\n",
    "  1. Advanced Data Transformation & Reshaping\n",
    "  2. Advanced Merging & Joining\n",
    "  3. Performance Optimization & Memory Management\n",
    "  4. Custom Extensions & Advanced Functionality\n",
    "  5. Real-World Challenge - Retail Analytics\n",
    "\n",
    "✓ KEY LEARNINGS:\n",
    "  • Pivot/Melt for flexible data reshaping\n",
    "  • Hierarchical indexing for complex data organization\n",
    "  • Advanced groupby operations with transform/agg\n",
    "  • Complex merges with duplicate/conflict handling\n",
    "  • Time-based joins with AsOf and time-windows\n",
    "  • Memory optimization through dtype management\n",
    "  • Performance gains through vectorization\n",
    "  • Custom pandas accessors for domain operations\n",
    "  • Method chaining for readable pipelines\n",
    "  • Real-world analytics workflow\n",
    "\n",
    "✓ BEST PRACTICES:\n",
    "  • Always vectorize - Never use iterrows\n",
    "  • Validate data quality before analysis\n",
    "  • Use method chaining for readability\n",
    "  • Document all custom operations\n",
    "  • Optimize memory for large datasets\n",
    "  • Test merge operations for data integrity\n",
    "\n",
    "📊 RESOURCES FOR FURTHER LEARNING:\n",
    "  • https://pandas.pydata.org/docs/\n",
    "  • https://pandas.pydata.org/docs/user_guide/\n",
    "  • Real dataset exploration and practice\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Thank you for using this comprehensive Pandas guide!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Week1_env (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
